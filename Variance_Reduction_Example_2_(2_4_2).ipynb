{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2F91LZjaY8HbXuYPt1X06",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyhu25/Variance-Reduction-Code/blob/main/Variance_Reduction_Example_2_(2_4_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "bBwmgV2gexDp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import linalg\n",
        "import math\n",
        "import scipy\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import rv_continuous, rv_discrete\n",
        "from scipy.stats._distn_infrastructure import rv_frozen\n",
        "from scipy.special import logsumexp\n",
        "import scipy.integrate\n",
        "import warnings\n",
        "import sys\n",
        "import statistics\n",
        "import pandas as pd\n",
        "from IPython.display import display, Math, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Bayesian model selection in non-conjugate linear regression"
      ],
      "metadata": {
        "id": "Y57cOJC_htca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider $y = XŒ≤ + œµ$, where $y = (y_{1} ,... , y_{N})$ a column vector of observations, $X$ is an $N \\times p$ design matrix, $Œ≤ = (\\beta_{1},...,\\beta_{p})$ is a column vector of regression coefficients, and $œµ \\sim N(0, \\sigma^2 I_{n})$ is the error vector.\n",
        "\n",
        "For Bayesian model selection, we seek to calculate posterior model probabilities $f(m|y) \\propto f(m)f(y|m)$ for every $m$ specified by $Œ≤_{m}$, where we arrange $p$ elements from $Œ≤$ with $2^p$ possible sets (the power set) for each design matrix $X_{m}$. So, we need to calculate the marginal likelihood of y with respect to model m, given by $f(y|m) = ‚à´f(y|m, \\beta_{m})f(\\beta_{m}|m)dŒ≤_{m}$, such that $f(y|m, \\beta_{m})$ is the conditional likelihood function of y on $m$ and $\\beta_{m}$, and $f(\\beta_{m}|m)$ is the prior density of model $m$.\n",
        "\n",
        "In this example, we seek to estimate the density of $f(y|m, Œ≤_{m})$ analytically. So, we are implicitly finding the expectation of $f(y|m, \\beta_{m})$, or $E_{q}(f(y|m, \\beta_{m})$ -- in this example, we set the function $F(x) = f(y|m, \\beta_{m})$. To do so, note that we have two priors in this example: one on $g$ and one on $\\beta_{m}|g, m$. Let N = 50 datapoints, with predictors $X_{i} \\sim N(0,1), i = 1,...,4$ and response $Y \\sim N(4X_{3} + 4X_{4}, 2.5 ^ 2)$. Suppose we choose a proposal density for $\\beta_{m}$ in the form\n",
        "\n",
        "$$\n",
        "q(\\beta_{m}) = \\sum_{i=1}^{K} w_{i}N(\\beta_{m}|0, g_{i}(X_{m}^{\\top}X_{m})^{-1})\n",
        "$$\n",
        "\n",
        "such that $\\sum_{i=1}^{K} w_{i} = 1, 0 < w_{i} < 1$\n",
        "\n",
        "Note that the g-prior is defined as $p(g) = (1+g)^{-2}, g > 0$ in this example."
      ],
      "metadata": {
        "id": "DR-d_TYRhtmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will simulate the g-prior by using the Lomax distribution directly from [SciPy](https://scipy.github.io/devdocs/reference/generated/scipy.stats.lomax.html), given below as:\n",
        "\n",
        "$f(x|c) = \\frac{c}{(1+x)^{c+1}}$\n",
        "\n",
        "So the g-prior is actually a Lomax distribution with shape parameter($c$) equal to 1.\n"
      ],
      "metadata": {
        "id": "qOqUrRwBh6b8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numbers for this example will be from the $X_{123}$ model, which means that the first three covariates $(1,2,3)$, denoted as $m$ in the paper, will be used in the model. However, I will run all of the possible covariates combinations for comparison of the algorithm."
      ],
      "metadata": {
        "id": "r67r8R4oh2Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the X and Y matrices\n",
        "## N and p are set from the paper\n",
        "N = 50\n",
        "p = 4\n",
        "# Generate X first\n",
        "mu = 0\n",
        "sigma = 1\n",
        "X_1 = np.random.normal(mu, sigma, size = N)\n",
        "X_2 = np.random.normal(mu, sigma, size = N)\n",
        "X_3 = np.random.normal(mu, sigma, size = N)\n",
        "X_4 = np.random.normal(mu, sigma, size = N)\n",
        "X = np.column_stack((X_1, X_2, X_3, X_4))\n",
        "# Generate Y, note the distribution of Y is defined above\n",
        "y = []\n",
        "for i in range(len(X_1)):\n",
        "  Y_i = np.random.normal(loc = 4 * X_3[i] + 4 * X_4[i], scale = 2.5)\n",
        "  y.append(Y_i)"
      ],
      "metadata": {
        "id": "8SH8cAMfh1zD"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beta_generation(N, g_prior_function, m):\n",
        "  betas = []\n",
        "  g = []\n",
        "  for i in range(N):\n",
        "    # If prior function is a scipy frozen RV. can directly sample from its RVs attribute call\n",
        "    if isinstance(g_prior_function, rv_frozen):\n",
        "      # Generate g prior first\n",
        "      g_i = g_prior_function.rvs(size = 1)\n",
        "      # Then generate beats\n",
        "      beta_i = np.random.multivariate_normal(mean = [0 for i in range(len(m))],\n",
        "                                           cov = g_i * np.identity(len(m)))\n",
        "      betas.append(beta_i.tolist())\n",
        "      g.append(g_i)\n",
        "    else:\n",
        "    # Otherwise, use a custom g prior function (used for clipping), and have to use correct function calls for that\n",
        "      g_i = g_prior_function(size = 1)\n",
        "      beta_i = np.random.multivariate_normal(mean = [0 for i in range(len(m))],\n",
        "                                           cov = g_i * np.identity(len(m)))\n",
        "      betas.append(beta_i.tolist())\n",
        "      g.append(g_i)\n",
        "    # sample beta from multivariate normal, with 0 mean and gI(m) covariance matrix\n",
        "\n",
        "  # 1st element is the simulated betas, 2nd element is the simulated g values\n",
        "  return betas, g"
      ],
      "metadata": {
        "id": "dFskKbIyh104"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that m is given as a string in this argument. This matches with the notation in the paper\n",
        "# if m = \"123\", then variables 1, 2, and 3 are chosen as the data matrix\n",
        "m = \"123\"\n",
        "A = beta_generation(1000, scipy.stats.lomax(c=1), m)\n",
        "\n",
        "# In this example, K(number of Gaussians to generate conditional distribution of Beta_m) is equal to 4\n",
        "K = 4"
      ],
      "metadata": {
        "id": "nltHSxxAiX5-"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that we need further add assumptions to recover the $\\hat\\Sigma_{j}= \\hat g_{i}I_{(m)}$ covariance structure in the EM algorithm. Although I can see that the $\\hat\\Sigma_{j}$ covariance matrices have similar diagonal elements and relatively small non-diagonal values, I still cannot infer what $\\hat g_{i}$ is from each $\\hat \\Sigma_{j}$. We can still sample from the covariance matrices $\\Sigma_{j}$, but the proposal density would now take the form of $q(\\hatŒ≤) = \\sum_{j=1}^{K} \\hat w_{j}N(\\hatŒ≤|0, \\hat\\Sigma_{j})$, and perform the same linear transformation $\\beta_{m} = (X_{m}^{T}X_{m})^{-\\frac{1}{2}}\\hat Œ≤$ to obtain the estimates of the coefficients.\n",
        "\n",
        "Here, I will derive the necessary steps for the M-step of this algorithm, from the joint log-likelihood of the EM algorithm, in order to enforce the $\\hat g_{i}I_{(m)}$ structure.\n",
        "\n",
        "Let $ùìõ(g_{j})$ be the joint log-likelihood of the datapoints, conditional on the fact that the covariance matrix for the Gaussian is $g_{j}I_{(m)}$.\n",
        "\n",
        "$$ùìõ(g_{j}) = \\sum_{i=1}^{N}r_{ij} log N(Œ≤_{i}|\\mu_{j} = 0_{m}, g_{j}I_{(m)})$$\n",
        "\n",
        "$$= \\sum_{i=1}^{N} r_{ij} log[(2œÄ)^{-m/2} |g_{j}I_{m}|^{-1/2}exp(-\\frac{1}{2}\\beta_{i}^{T}(g_{j}I_{m})^{-1}\\beta_{i})]$$\n",
        "\n",
        "$$ = \\sum_{i=1}^{N} r_{ij} [-\\frac{m}{2}log(2\\pi g_{j}) - \\frac{1}{2g_{j}}\\beta_{i}^{T}\\beta_{i}]$$\n",
        "\n",
        "$$ = -\\frac{m}{2}log(2\\pi g_{j})(\\sum_{i=1}^{N} r_{ij}) - \\frac{1}{2g_{j}}\\sum_{i=1}^{N}r_{ij}||\\beta_{i}||^{2}$$\n",
        "\n",
        "Note that the term $|g_{j}I_{m}|$ is equivalent to $g_{j}^{m}|I_{m}|$, due to the property of a determinant with a scalar of a matrix. Hence, the term $log[(2\\pi)^{-\\frac{m}{2}}|g_{j}I_{m}|^{-\\frac{1}{2}}]$ can be simplified to $-\\frac{m}{2}log(2\\pi g_{j})$\n",
        "\n",
        "Take derivative with respect to $g_{j}$ and set to $0$ to solve for $\\hat g_{j}$\n",
        "\n",
        "$$\\frac{dùìõ(g_{j})}{dg_{j}} = -\\frac{m}{2g_{j}}\\sum_{i=1}^{N}r_{ij} + \\frac{1}{2g_{j}^{2}}\\sum_{i=1}^{N}r_{ij}||Œ≤_{i}||^{2} = 0$$\n",
        "\n",
        "$$ -mg_{j}\\sum_{i=1}^{N}r_{ij} + \\sum_{i=1}^{N}r_{ij}||Œ≤_{i}||^{2} = 0$$\n",
        "\n",
        "$$\\hat g_{j} = \\frac{\\sum_{i=1}^{N}r_{ij}||Œ≤_{i}||^{2}}{m\\sum_{i=1}^{N}r_{ij}}$$\n",
        "\n",
        "Now, instead of updating $\\Sigma_{j}$, we use this to update $g_{j}$ and maintain the diagonal structure of the $\\Sigma_{j}$, as it was presented in the paper.\n",
        "\n",
        "I will write a new EM algorithm below for the change. Note that I will also add some quality of life changes (sometimes depending on the values drawn from the priors I can have extremely small values for $g_{i}$ or $N(0, gI_{m})$. So I will use the log trick(adding log to the expression and then exponentiate to cancel out the log) for numerical stability purposes."
      ],
      "metadata": {
        "id": "-TLbvgWZicRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EM_algorithm_Gaussians_diagonal_matrix(K, list_betas, p, g_sampler, N_iterations, update_mu, only_print_final = False,\n",
        "                           init_means=None, init_covs=None):\n",
        "    for i in range(len(list_betas)):\n",
        "        if len(list_betas[i]) != p:\n",
        "            warnings.warn(\"Dimensions of betas and p do not match.\")\n",
        "            sys.exit(1)\n",
        "    w_i = np.random.dirichlet(np.ones(K)).tolist()\n",
        "    initial_w_i = w_i.copy()\n",
        "    g_i = [0 for _ in range(K)]\n",
        "    # Log Likelihood list for convergence diagnostics -- should always be nondecreasing\n",
        "    log_likelihoods_list = []\n",
        "    if init_means is not None and len(init_means) == K:\n",
        "        mu_list = init_means\n",
        "    elif init_means is not None and len(init_means) != K:\n",
        "        warnings.warn(\"Number of means and K do not match.\")\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        mu_list = [[0 for _ in range(p)] for _ in range(K)]\n",
        "    if init_covs is not None:\n",
        "        sigma_list = init_covs\n",
        "    elif hasattr(g_sampler, \"rvs\"):\n",
        "        g_i = g_sampler.rvs(size = K)\n",
        "        sigma_list = [g * np.identity(p) for g in g_i]\n",
        "    else:\n",
        "        g_i = g_sampler(K)\n",
        "        sigma_list = [g * np.identity(p) for g in g_i]\n",
        "    # Initialize the initial g_i for comparison of algorithm\n",
        "    initial_g_i = g_i.copy()\n",
        "    for v in range(N_iterations):\n",
        "      ## Adding total log_likelihood counter for measure of model performance\n",
        "        ll_total = 0.0\n",
        "        responsibilities = np.empty(shape=(len(list_betas), K))\n",
        "        # E-step\n",
        "        for i in range(len(list_betas)):\n",
        "          # initiate beta\n",
        "            beta_i = list_betas[i]\n",
        "          # calculate likelihoods\n",
        "          ## using log likelihood for numerical stability purposes\n",
        "          ### Indeterminate values when calculating EM, especially when g is small, due to abnormally large or small PDFs, which can cause 0-near or\n",
        "          ### super-large denominators. This in turn causes NAN values, which causes issues with the algorithm\n",
        "            log_likelihoods = [\n",
        "                math.log(w_i[j]) + scipy.stats.multivariate_normal.logpdf(\n",
        "                    beta_i, mean=mu_list[j], cov=sigma_list[j], allow_singular=True\n",
        "                )\n",
        "                for j in range(K)\n",
        "            ]\n",
        "            denom = scipy.special.logsumexp(log_likelihoods)\n",
        "            ll_total += denom\n",
        "            for j in range(K):\n",
        "              # Exponentiate to remove the log.\n",
        "              responsibilities[i][j] = math.exp(log_likelihoods[j] - denom)\n",
        "        # M-step\n",
        "        for i in range(K):\n",
        "            r_i = responsibilities[:, i]\n",
        "            total_r_i = np.sum(r_i)\n",
        "            if update_mu == True:\n",
        "              # Calculate Mus\n",
        "              mu_new = np.sum([\n",
        "                  r_i[j] * np.array(list_betas[j]) for j in range(len(list_betas))\n",
        "              ], axis=0) / total_r_i\n",
        "              mu_list[i] = mu_new\n",
        "              # Calculate g_is based on new mus, and previously outlined algorithm\n",
        "              g_hat_new = np.sum([r_i[j] * np.linalg.norm(np.array(list_betas[j]) - mu_new) ** 2 for j in range(len(list_betas))]) / (p * total_r_i)\n",
        "              g_i[i] = g_hat_new\n",
        "              # recalculate sigmas\n",
        "              sigma_list[i] = g_hat_new * np.identity(p)\n",
        "            else:\n",
        "              # No update mu -- just calculate sigmas and g_is\n",
        "              g_hat_new = np.sum([r_i[j] * np.linalg.norm(np.array(list_betas[j])) ** 2 for j in range(len(list_betas))]) / (p * total_r_i)\n",
        "              g_i[i] = g_hat_new\n",
        "              sigma_list[i] = g_hat_new * np.identity(p)\n",
        "        # Recalculate weights\n",
        "        w_i = [float(np.sum(responsibilities[:, i]) / len(list_betas)) for i in range(K)]\n",
        "        if only_print_final == False:\n",
        "          # print weights at each iteration of process\n",
        "          print(f\"Iteration {v+1}: Weights: {w_i}: Log Likelihood: {ll_total}\")\n",
        "          log_likelihoods_list.append(np.float64(ll_total))\n",
        "        else:\n",
        "          # Print only last step\n",
        "          if v == N_iterations - 1:\n",
        "            print(f\"Iteration {v+1}: Final Weights: {w_i}: Log Likelihood : {ll_total}\")\n",
        "            log_likelihoods_list.append(np.float64(ll_total))\n",
        "    # Return final weights, initial weights, list of vector of mu, initial g_i (sampled from g-prior),\n",
        "    ## Final g_prior calculated from EM, and log-likelihood list (for convergence and debugging purposes)\n",
        "    return w_i, initial_w_i, mu_list, initial_g_i, g_i, log_likelihoods_list"
      ],
      "metadata": {
        "id": "LfQ-msOoifF0"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_update_mu_diagonal_sigma = EM_algorithm_Gaussians_diagonal_matrix(K, A[0], len(m), scipy.stats.lomax(c=1), 20, False)\n",
        "print(f\"Final weights:{no_update_mu_diagonal_sigma[0]}\")\n",
        "print(f\"Initial weights: {no_update_mu_diagonal_sigma[1]}\")\n",
        "print(f\"Final mean vectors: {no_update_mu_diagonal_sigma[2]}\")\n",
        "print(f\"initial g_j: {no_update_mu_diagonal_sigma[3]}\")\n",
        "print(f\"final g_j: {no_update_mu_diagonal_sigma[4]}\")\n",
        "# Plot the log likelihood as it is calculated through iterations\n",
        "plt.plot(no_update_mu_diagonal_sigma[5])\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Log Likelihood\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "7wZx56LXihuM",
        "outputId": "1b8ba416-9400-496a-efe8-b7b6448b0350"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Weights: [0.060585322876713685, 0.14923536949843932, 0.5335950216660299, 0.2565842859588172]: Log Likelihood: -5744.1845356338035\n",
            "Iteration 2: Weights: [0.05849451953718102, 0.1314039169791298, 0.50313089974802, 0.30697066373566917]: Log Likelihood: -5149.536724981347\n",
            "Iteration 3: Weights: [0.060898069533549835, 0.12410319650652275, 0.48018769186149807, 0.3348110420984293]: Log Likelihood: -5132.124474413553\n",
            "Iteration 4: Weights: [0.061648022817631154, 0.12029485392968842, 0.4623676912057953, 0.3556894320468852]: Log Likelihood: -5126.01621706667\n",
            "Iteration 5: Weights: [0.06128494405489169, 0.11776521418535034, 0.4484804539802075, 0.3724693877795504]: Log Likelihood: -5122.110653509478\n",
            "Iteration 6: Weights: [0.060352183219789524, 0.11593418980368067, 0.4372456580422656, 0.3864679689342642]: Log Likelihood: -5119.288286573196\n",
            "Iteration 7: Weights: [0.05917317453559711, 0.11460876871678141, 0.4278321236902838, 0.3983859330573377]: Log Likelihood: -5117.138427533115\n",
            "Iteration 8: Weights: [0.05792391933068673, 0.11370044624266705, 0.4197300292434905, 0.4086456051831557]: Log Likelihood: -5115.455726716357\n",
            "Iteration 9: Weights: [0.05669393984243965, 0.11314973497810805, 0.4126150806014685, 0.41754124457798375]: Log Likelihood: -5114.113492738513\n",
            "Iteration 10: Weights: [0.05552394537056901, 0.11290570377914155, 0.40626917671645596, 0.42530117413383345]: Log Likelihood: -5113.023826449854\n",
            "Iteration 11: Weights: [0.054427991052640214, 0.11292173397545623, 0.4005376442432513, 0.43211263072865225]: Log Likelihood: -5112.122609940621\n",
            "Iteration 12: Weights: [0.053406312891797494, 0.1131554858860575, 0.3953057974348524, 0.4381324037872927]: Log Likelihood: -5111.362375525542\n",
            "Iteration 13: Weights: [0.05245260263119747, 0.11356943835420102, 0.3904855182002267, 0.44349244081437483]: Log Likelihood: -5110.707955761915\n",
            "Iteration 14: Weights: [0.05155798178044937, 0.11413112500405544, 0.38600719900705305, 0.44830369420844207]: Log Likelihood: -5110.133354411635\n",
            "Iteration 15: Weights: [0.05071305505531504, 0.11481301060850793, 0.3818146760448533, 0.45265925829132375]: Log Likelihood: -5109.619358670196\n",
            "Iteration 16: Weights: [0.04990888391403333, 0.11559213001767316, 0.3778618919816664, 0.456637094086627]: Log Likelihood: -5109.151714328485\n",
            "Iteration 17: Weights: [0.04913737968349255, 0.11644960934733192, 0.3741105937925852, 0.4603024171765903]: Log Likelihood: -5108.7197551576655\n",
            "Iteration 18: Weights: [0.0483914026609943, 0.11737015143921409, 0.37052867375392046, 0.4637097721458711]: Log Likelihood: -5108.315393173656\n",
            "Iteration 19: Weights: [0.04766472507581851, 0.11834153421161793, 0.36708892831287265, 0.46690481239969084]: Log Likelihood: -5107.932386514608\n",
            "Iteration 20: Weights: [0.046951941433515276, 0.11935414766688018, 0.36376810239658186, 0.46992580850302273]: Log Likelihood: -5107.565814718682\n",
            "Final weights:[0.046951941433515276, 0.11935414766688018, 0.36376810239658186, 0.46992580850302273]\n",
            "Initial weights: [0.037158711329024804, 0.25930203688872805, 0.5504836149640961, 0.1530556368181511]\n",
            "Final mean vectors: [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
            "initial g_j: [11.67919849  0.03801412  4.76455738  0.22261625]\n",
            "final g_j: [1.12076114e+02 3.82691638e-02 4.40053315e+00 5.27649553e-01]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Log Likelihood')"
            ]
          },
          "metadata": {},
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are ready to run the Independent Metropolis-Hastings algorithm. Again, note that our proposal density is noted as $q(\\hatŒ≤) = \\sum_{j=1}^{K} \\hat w_{j}N(\\hatŒ≤|0, \\hat g_{i}I_{(m)})$ from the EM algorithm, with estimates $\\{\\hat w_j, \\hat g_j\\}_{j=1}^{K}$. Then, we use the transformation $\\beta_{m} = (X_{m}^{T}X_{m})^{-\\frac{1}{2}}\\hat\\beta_{m}$ to obtain the estimates of the coefficients.\n",
        "\n",
        "We hope that these proposed samples matches the priors given below:\n",
        "\n",
        "$$\\beta_{m}|g, m \\sim N(0, g(X_{m}^{T}X_{m})^{-1}) , p(g) = (1+g)^{-2}, g > 0$$\n",
        "\n",
        "So $\\pi(x)$, the target density in this example, is the conditional prior distribution of $\\beta_{m}$. So for each proposed sample $y$ and current sample $x$ in the MCMC, the acceptance ratio will be:\n",
        "\n",
        "$$\\alpha(x, y) = min (1, \\frac{f_{\\beta_{m}}(y|m)q_{\\beta_{m}}(x)}{f_{\\beta_{m}}(x|m)q_{\\beta_{m}}(y)} ) = min(1, \\frac{f_{\\beta_{m}}(y|m)(X_{m}^{T}X_{m})^{-1}q_{\\hatŒ≤_{m}}(x)}{f_{\\beta_{m}}(x|m)(X_{m}^{T}X_{m})^{-1}q_{\\hat\\beta_{m}}(y)}= min(1, \\frac{f_{\\beta_{m}}(y|m) (X^{T}_{m}X_{m})^{-1/2} \\sum_{j=1}^{K} \\hat w_{j} N(x|0_{m}, \\hat g_{j}I_{(m)} )}{f_{\\beta_{m}}(x|m) (X^{T}_{m}X_{m})^{-1/2} \\sum_{j=1}^{K} \\hat w_{j} N(y|0_{m}, \\hat g_{j}I_{(m}))})$$\n",
        "\n",
        "$$ = min(1, \\frac{f_{\\beta_{m}}(y|m) \\sum_{j=1}^{K} \\hat w_{j} N(x|0_{m}, \\hat g_{j}I_{(m)} )}{f_{\\beta_{m}}(x|m) \\sum_{j=1}^{K} \\hat w_{j} N(y|0_{m}, \\hat g_{j}I_{(m}))})$$\n",
        "\n",
        "2 Things to note:\n",
        "\n",
        "1. For the target density $\\pi(.) = f_{\\beta_{m}}( .|m) = \\int_{0}^{‚àû} f(\\beta_{m}|m, g)p(g)dg = \\int_{0}^{‚àû} N(0, g(X^{T}_{m}X_{m})^{-1}p(g)dg$, which is the integral of the joint product of the two priors given on the support of $g$ (from $0$ to $‚àû$). We can approximate this integral by MCMC, where we take $L$ samples, and approximate the density by averaging $L$ products of the multivariate normal density, and the g-prior density. So, $\\pi(.) ‚âà \\frac{1}{L}\\sum_{i=1}^{L}N(.|0_{m}, g_{i}(X_{m}^{T}X_{m})^{-1}), g_{i} ‚àº p(g) = (1+g)^{-2}$\n",
        "2. We have expressions for the analytical value of $f(y|m)$ and the expectation of the F-function (which is the conditional distribution of y on m and $\\beta_{m}$, or $f(y|m, \\beta_{m})$) $E_{q(\\beta_{m})} (f(y|m, \\beta_{m})$. These  expression are given in the appendix of the paper, as seen [here](https://arxiv.org/pdf/2406.17699#page=52)."
      ],
      "metadata": {
        "id": "284WoCYMjZie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the form given in the paper, we are trying to estimate $F$ function -- the marginal likelihood of $y$, on $m$ : $f(y|m)$ given by the integral:\n",
        "\n",
        "$$f(y|m) = ‚à´f(y|m, Œ≤_{m})f(Œ≤_{m}|m)dŒ≤_{m} = ‚à´f(y|m, Œ≤_{m})[N(0, g(X_{m}^{T}X_{m})^{-1}p(g)dg]dŒ≤_{m} = ‚à´(2 \\pi œÉ^{2})^{-\\frac{N}{2}}exp[-\\frac{1}{2œÉ_{2}}‚àë_{i=1}^{N}(y_{i} - x_{i}^{T}Œ≤_{m}) ^{2}]N(0, g(X_{m}^{T}X_{m})^{-1})p(g)d\\beta_{m}dg = \\\\ C\\int p(g)[\\int (2\\pi g)^{-\\frac{d}{2}}|(X_{m}^{T}X_{m})^{-1}|^{-\\frac{1}{2}}exp[\\frac{1}{œÉ_{2}} \\sum_{i=1}^{N}y_{i}x_{i}^{T}Œ≤_{m}- \\frac{1}{2}(\\frac{1}{œÉ^{2}}+\\frac{1}{g})Œ≤_{m}^{T}(X_{m}^{T}X_{m})Œ≤_{m}]dŒ≤_{m}]dg\n",
        "= \\\\ C‚à´(1+\\frac{g}{œÉ^{2}})^{-\\frac{d}{2}}exp[\\frac{1}{2 œÉ^{2}}\\frac{g}{g+œÉ^{2}}(\\sum_{i=1}^{N}y_{i}x_{i}^{T}(X_{m}^{T}X_{m})^{-1}(\\sum_{i=1}^{N}y_{i}x_{i}^{T})^{T}](1+g)^{-2}dg =\n",
        "\\\\C‚à´(1+\\frac{g}{\\sigma^{2}})^{-\\frac{d}{2}}exp[\\frac{1}{2 \\sigma^{2}} \\frac{g}{g + \\sigma^{2}}(yX_{m})(X_{m}^{T}X_{m})^{-1}(yX_{m})^{T}](1+g)^{-2}dg $$\n",
        "\n",
        "for $i = 1,...,N$ observations corresponding to $(x_{i}, y_{i})$\n",
        "\n",
        "Where $C = (2\\pi œÉ^{2})^{-\\frac{N}{2}}exp[-\\frac{1}{2œÉ^{2}} \\sum_{i=1}^{N} y_{i}^{2}]$, and $x_{i}$ is the $i$th row of data matrix $X_{m}$.\n",
        "\n",
        "Since this is a univariate integral, we can explicitly calculate the marginal likelihood\n",
        "\n",
        "With the proposed variance reduction scheme, we can approximate this integral, with $N$ samples, by calculating its expectation with regards to the target density $\\pi(v)$, along with reducing the sampler's variance, by using the sum given from integrating the marginal likelihood $f(y|m, Œ≤_{m}$) with respect to the proposal density $q(Œ≤_{m})$, with the full results given [here](https://arxiv.org/pdf/2406.17699#page=52):\n",
        "\n",
        "$$ùîº_{q(Œ≤_{m})}(f(y|m, Œ≤_{m})) = \\int f(y|m, Œ≤_{m})q(Œ≤_{m})dŒ≤_{m} \\\\\n",
        " = C \\sum_{j=1}^{K} w_{j}(1+\\frac{g_{j}}{œÉ^{2}})^{-\\frac{p}{2}}exp[\\frac{1}{2œÉ^{2}} \\frac{g_{j}}{g_{j}+œÉ^{2}} (\\sum_{i=1}^{N}y_{i}x_{i})^{T} (X_{m}^{T}X_{m})^{-1}(\\sum_{i=1}^{N}y_{i}x_{i})^{T}] \\\\\n",
        " = C \\sum_{j=1}^{K}w_{j}(1+\\frac{g_{j}}{\\sigma^{2}})^{-\\frac{p}{2}}exp[\\frac{1}{2\\sigma^{2}}\\frac{g}{g+\\sigma^{2}}(yX_{m})(X_{m}^{T}X_{m})^{-1}(yX_{m})^{T}]$$\n",
        "\n",
        "\n",
        "for $j = 1,...,K$ Gaussians used in the proposal density $q(\\beta_{m})$ and its g-prior $p(g)$, $(x_{i}, y_{i})_{i=1}^{N}$ generated samples for the original data $(X, y)$, $C = (2\\pi œÉ^{2})^{-\\frac{N}{2}}exp[-\\frac{1}{2œÉ^{2}} \\sum_{i=1}^{N} y_{i}^{2}]$ the normalizing constant, $œÉ^{2}$ the scaling factor for the multivariate error term $œµ \\sim N(0, œÉ^{2}I_{m})$, and $p$ is the dimension of the $\\beta_{m}$ or the number of variables selected."
      ],
      "metadata": {
        "id": "yC_UrSCtjbm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike the previous example, we need to create a separate conditional Markov Chain sampling algorithm for Bayesian linear regression.\n",
        "\n",
        "In Bayesian literature, there is a Gibbs-sampling algorithms using g-priors for linear regression on the $g$ prior value wich generates the $\\beta_{m}$ from its conditional prior on $g$ and $m$ (recall that the prior is $\\beta_{m}|g, m \\sim N(0, gI_{m})$\n",
        "\n",
        "Commonly, a prior used for $\\sigma^{2}$ in linear regression is the Jeffrey's prior. The Jeffrey's prior is a non-informative prior used for any parameter space. In [Gelman](https://sites.stat.columbia.edu/gelman/book/BDA3.pdf#page=62), the Jeffrey's prior for a parameter $\\theta$ is formally defined as the square root of the Fisher information $J(\\theta)$ of a distribution:\n",
        "\n",
        "$$p(\\theta) \\propto |J(\\theta)|^{\\frac{1}{2}} = |ùîº((\\frac{dlogP(y|\\theta)}{d\\theta}|Œ∏)^{2})|^{\\frac{1}{2}} = |-ùîº(\\frac{d^{2}log P(y|\\theta)}{d\\theta^{2}} )|^{\\frac{1}{2}}$$\n",
        "\n",
        "where $P(y|\\theta)$ is the density of the response variable $y$, conditional on $\\theta$\n",
        "\n",
        "Sometimes, a Jeffrey's prior for a distribution is an improper distribution, as is the case for the Jeffrey's prior for $\\sigma^{2}$. The Jeffrey's prior on $\\sigma^{2}$ for a Gaussian distribution $p(\\sigma^{2})$ is proportionate to $\\frac{1}{\\sigma^{2}}$, which means that the Jeffrey's prior is a uniform prior on the parameter space of $\\sigma^{2}$. However, the prior $p(\\sigma^{2})$ does not integrate to $1$ over $(0, ‚àû)$. Hence, Jeffrey's prior is an improper distribution. It is also the limit of an inverse gamma\n",
        "\n",
        "Nevertheless, the Jeffrey's prior is commonly used because it is a uniform flat prior on $\\sigma^{2}$, which is represents no prior knowledge of the distribution of $\\sigma^{2}$, and it has a nice conjugate form for the posterior, conditional on $y, X_{m}$, and $g$, which is important because we sample $\\sigma^{2}$ from its posterior, instead of its prior.\n",
        "\n",
        "Using the Jeffrey's prior on $\\sigma^{2}$ ($p(œÉ) \\propto \\frac{1}{\\sigma^{2}}$), we can find the form of the posterior distribution $p(\\sigma^{2}|y, X_{m})$. This posterior distribution is an inverse-Gamma, and its full form listed below, from [Hoff](https://sites.math.rutgers.edu/~zeilberg/EM20/Hoff.pdf#page=162)\n",
        "\n",
        "$$p(\\sigma^{2}|y, X_{m}, g) \\sim inv-Gamma(\\frac{v_{0}+n}{2}, \\frac{v_{0}\\sigma_{0}^{2} + SSR_{g}}{2}) $$\n",
        "\n",
        "Where $SSR_{g} = y^{T}(I - \\frac{g}{g+1}X_{m}(X_{m}^{T}X_{m})^{-1}X^{T})y$.\n",
        "\n",
        "So, with the Gibbs sampler algorithm, we sample $Œ≤$ and $\\sigma^{2}$\n",
        "\n",
        "\n",
        "One more thing note is the paper's construction of $y \\sim N(4X_{3} + 4X_{4}, 2.5^{2})$ implies that the true $Œ≤$ is the vector $(0, 0, 4, 4)$"
      ],
      "metadata": {
        "id": "dYnpOK_Hjd04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_function_y_likelihood(sigma_error, Y_response_vector, X_data_matrix):\n",
        "  # First to see if dimeinsions match\n",
        "  # number of rows of X_data_matrix and length of Y_response_vector should match\n",
        "  if X_data_matrix.shape[0] != len(Y_response_vector):\n",
        "    warnings.warn(\"Dimensions of X_data_matrix and Y_response_vector do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Calculate constant first using logarithms for numerical stability\n",
        "  ## Find number of samples\n",
        "  N = len(Y_response_vector)\n",
        "  ## Find dimension of Gaussians\n",
        "  d = X_data_matrix.shape[1]\n",
        "  ## Calculate log constant\n",
        "  C = (2 * math.pi * (sigma_error ** 2)) ** (-N/2) * math.exp((-1 / (2 * sigma_error ** 2)) * float(np.sum([y ** 2 for y in Y_response_vector])))\n",
        "  # Numerically integrate\n",
        "  ## Set up function\n",
        "  ### Evaluate the summation of y_i x_i^T using a dot product\n",
        "  yixi = Y_response_vector @ X_data_matrix\n",
        "  ### inverse of X^TX\n",
        "  inv_X = np.linalg.inv(X_data_matrix.T @ X_data_matrix)\n",
        "  ## Time to integrate\n",
        "  ### Write function within the quad() function call using lambda\n",
        "  integral_value, error = scipy.integrate.quad(\n",
        "    lambda g: ((1 + g / (sigma_error ** 2)) ** (-d/2)) * math.exp( (g / (2 * (sigma_error ** 2) * (g + sigma_error ** 2))) * yixi @ inv_X @ yixi.T) * ((1 + g) ** (-2)),\n",
        "    0, np.inf)\n",
        "  # Return the constant times the integral\n",
        "  return C * integral_value"
      ],
      "metadata": {
        "id": "ka4qnFpqjgGt"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expectation_f_function_y_likelihood(weights, g_prior_samples, X_data_matrix, Y_response_vector, sigma_error):\n",
        "  # First check to see dimensions match\n",
        "  ## Weights and g_prior should have same dimension -- K\n",
        "  if len(weights) != len(g_prior_samples):\n",
        "    warnings.warn(\"Dimensions of weights and g_prior do not match.\")\n",
        "    sys.exit(1)\n",
        "  ## number of rows of X_data_matrix and length of Y_response_vector should match\n",
        "  if X_data_matrix.shape[0] != len(Y_response_vector):\n",
        "    warnings.warn(\"Dimensions of X_data_matrix and Y_response_vector do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Calculate the constant first\n",
        "  ## Find the number of samples\n",
        "  N = len(Y_response_vector)\n",
        "  ## Find the dimensions of multivariate normals\n",
        "  d = X_data_matrix.shape[1]\n",
        "  ## Calculate the constant\n",
        "  C = (2 * math.pi * (sigma_error ** 2)) ** (- N / 2) * math.exp((-1 / (2 * sigma_error ** 2)) * float(np.sum([y ** 2 for y in Y_response_vector])))\n",
        "  # then sum over the K elements (number of mixtures)\n",
        "  sum = 0\n",
        "  ## Number of distributions used in mixture (K)\n",
        "  K = len(weights)\n",
        "  ## Dimension of the\n",
        "  for i in range(K):\n",
        "    g_i = g_prior_samples[i]\n",
        "    w_i = weights[i]\n",
        "    sum += w_i * (1 + g_i / (sigma_error** 2)) ** (- d / 2) * math.exp((1 / (2 * sigma_error ** 2)) * (g_i / (g_i + sigma_error ** 2)) *\n",
        "                                                                (Y_response_vector @ X_data_matrix) @ # Y_i x_i^T summation\n",
        "                                                                np.linalg.inv(X_data_matrix.T @ X_data_matrix) @ # inverse of product of X^T X\n",
        "                                                                (Y_response_vector @ X_data_matrix).T) # Transpose of Y_i x_i^T summation\n",
        "  return C * sum"
      ],
      "metadata": {
        "id": "E279jfq_jhwy"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_pseudo_prior_estimation(z, L, X_data_matrix, g_priors):\n",
        "  # Check dimensions\n",
        "  ## Length of z must be the same as the number of columns of X_data_matrix\n",
        "  if len(z) != X_data_matrix.shape[1]:\n",
        "    warnings.warn(\"Dimensions of z and X_data_matrix do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Solve for (X^TX)^-1 first\n",
        "  ## Need to generate the g-priors first before feeding it through the function\n",
        "  covariance_matrix = np.linalg.inv(X_data_matrix.T @ X_data_matrix)\n",
        "  mean_pdf_calculation = np.mean([scipy.stats.multivariate_normal.pdf(z, mean =  [0 for _ in range(len(z))],\n",
        "                                                  cov = g_priors[i] * covariance_matrix) for i in range(L)])\n",
        "  return float(mean_pdf_calculation)"
      ],
      "metadata": {
        "id": "pfup3FcXjjS7"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SSR_g_calculation(g, X, y):\n",
        "  # Calculate \"quadratic\" form of X\n",
        "  quad_X = X @ np.linalg.inv(X.T @ X) @ X.T\n",
        "  return y.T @ (np.eye(quad_X.shape[0]) - (g / (g+1)) * quad_X) @ y"
      ],
      "metadata": {
        "id": "-5le_3Bejk7r"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_y_on_beta(beta, X, y, sigma):\n",
        "  # Calculate error first\n",
        "  error = (y - X @ beta).T @ (y - X @ beta)\n",
        "  # Constant\n",
        "  n = X.shape[0]\n",
        "  constant = 1 / math.sqrt( (2 * math.pi * (sigma ** 2)) ** n)\n",
        "  return constant * math.exp( (-1/(2 * (sigma ** 2))) * error)"
      ],
      "metadata": {
        "id": "iDy-ragwjmVW"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting columns in X data matrix based on string\n",
        "def X_data_matrix_from_m(X, m):\n",
        "  # Shift one left because of Python indices starting at 0\n",
        "  columns = [(int(i) - 1) for i in m]\n",
        "  return X[:,columns]"
      ],
      "metadata": {
        "id": "Mz8k-k4kjoyY"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CMC_gibbs_linear_regression(N_samples, burn_in, X_data_matrix, Y_response_vector, T_iterations, weights_g, g_i_vector):\n",
        "  # Sanity check: X_data_matrix and Y_response_vector dimensions line up\n",
        "  if X_data_matrix.shape[0] != len(Y_response_vector):\n",
        "    warnings.warn(\"Dimensions of X_data_matrix and Y_response_vector do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Define N for the posterior of Sigma -- note it is not the same as the N_samples\n",
        "  N = X_data_matrix.shape[0]\n",
        "  p = X_data_matrix.shape[1]\n",
        "  print((N, p))\n",
        "  mu_MC = []\n",
        "  g_vector = []\n",
        "  # Calculate the quadratic inverse of X^T X using Cholesky once\n",
        "  quad_inverse = np.linalg.inv(X_data_matrix.T @ X_data_matrix)\n",
        "  for i in range(T_iterations):\n",
        "    # First pick and fix from gs from mixture of Gaussians\n",
        "    chosen_g = np.random.choice(a = g_i_vector, p = weights_g)\n",
        "    g_vector.append(chosen_g)\n",
        "    # Also calculate the SSR_g first -- this will be needed for sampling of the sigmas\n",
        "    SSR_g = SSR_g_calculation(chosen_g, X_data_matrix, Y_response_vector)\n",
        "    # Calculate beta_OLS and g_ratio for beta posterior sampling\n",
        "    ## Use quad-inverse, since it is (X^{T} * X) ^{-1} * X^{T} y\n",
        "    beta_OLS = quad_inverse @ X_data_matrix.T @ Y_response_vector\n",
        "    g_ratio = chosen_g / (chosen_g+1)\n",
        "    # Initiate beta,sigma, and likelihood list inside each iteration -- need this for likelihood calculation\n",
        "    beta_list = []\n",
        "    sigma_2_list = []\n",
        "    likelihood_list = []\n",
        "    for j in range(burn_in + N_samples):\n",
        "      # generate the sigma first\n",
        "      sigma_2_iteration = scipy.stats.invgamma.rvs(a = N/2, scale = SSR_g/2, size = 1)\n",
        "      sigma_2_list.append(sigma_2_iteration)\n",
        "      # generate beta conditional on sigma\n",
        "      ## using the quad-inverse again for the covariance, since it is g * sigma^{2} * (X^T * X)^{-1}\n",
        "      beta_iteration = scipy.stats.multivariate_normal.rvs(mean = g_ratio * np.asarray([0 for _ in range(p)]),\n",
        "                                                           cov = g_ratio * sigma_2_iteration * quad_inverse,\n",
        "                                                           size = 1)\n",
        "      beta_list.append(beta_iteration)\n",
        "      # Calculate joint likelihood of data, conditional on each sigma and beta\n",
        "      ## Directly calculate using the PDF of multivariate normal\n",
        "      likelihood_iteration = scipy.stats.multivariate_normal.pdf(Y_response_vector - X_data_matrix @ beta_iteration.T, mean = [0 for _ in range(N)],\n",
        "                                                                 cov = sigma_2_iteration * np.eye(N))\n",
        "      likelihood_list.append(float(likelihood_iteration))\n",
        "    # Throw away burn_in_samples\n",
        "    beta_list = beta_list[burn_in:]\n",
        "    sigma_2_list = sigma_2_list[burn_in:]\n",
        "    likelihood_list = likelihood_list[burn_in:]\n",
        "    # Calculate the estimator means\n",
        "    mu_MC_i = np.mean(likelihood_list)\n",
        "    mu_MC.append(float(mu_MC_i))\n",
        "  return mu_MC, g_vector"
      ],
      "metadata": {
        "id": "_7pk0Rcvjqeo"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test = CMC_gibbs_linear_regression(1, 0, X_data_matrix_from_m(X, \"134\"), np.asarray(y), 1000, np.asarray(no_update_mu_diagonal_sigma[0]), np.asarray(no_update_mu_diagonal_sigma[4]))\n",
        "mu_MC_test = test[0]\n",
        "mu_MC_test_log10 = [-math.log10(mu_MC_test[i]) for i in range(len(mu_MC_test))]\n",
        "print(np.mean(mu_MC_test_log10))\n",
        "print(statistics.variance(mu_MC_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgPsD6_ujsZX",
        "outputId": "f129b2b6-bed8-4b38-8812-b2f1b908f9b0"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 3)\n",
            "76.58709750541276\n",
            "3.511217655111673e-134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(-math.log10(f_function_y_likelihood(2.5, np.asarray(y), X_data_matrix_from_m(X, \"123\"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_BYWlrRjuZF",
        "outputId": "b67c31d6-014b-404e-e09a-f1fb13ffe397"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77.16814464513664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are ready to initiate the Metropolis-Hastings algorithm for variance reduction. Again, note that the $F$ function is the marginal likelihood of response variable $y$, given the predictors chosen (as denoted as $m$). Hence, $$F(y) = f(y|\\beta_{m},X, œÉ) = (2\\pi\\sigma^{2})^{-\\frac{n}{2}}exp(-\\frac{(y-X\\beta_{m})^{T}(y-X\\beta_{m})}{2\\sigma^{2}})$$\n",
        "\n",
        "$$Œº_{IMCV} = \\frac{1}{n}\\sum_{i=1}^{n}{F(X_{i}) + Œ±(X_{i}, Y_{i})(F(Y_{i})-F(X_{i})) - (F(Y_{i}) - ùîº_{q}(F(Y_{i})))}$$"
      ],
      "metadata": {
        "id": "hB6E13Ypjxyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multivariate_linear_regression_metropolis(K, N_samples, T_iterations,\n",
        "                                              weights, g_j_hat, m, g_prior_function, X_data_matrix, Y_response_vector,\n",
        "                                              L, sigma_error):\n",
        "  # Sanity checks: make sure K, length of weights, and length of g_j_hat is the same\n",
        "  if len(weights) != len(g_j_hat) or len(weights) != K:\n",
        "    warnings.warn(\"Dimensions of weights, g_i, and K do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Make sure dimension of m, and columns of X_data_matrix are the same\n",
        "  if len(m) != X_data_matrix.shape[1]:\n",
        "    warnings.warn(\"Dimensions of m and X_data_matrix do not match.\")\n",
        "    sys.exit(1)\n",
        "  # reassign length of m to P if this passes.. issues with len(m) not acting right\n",
        "  p = X_data_matrix.shape[1]\n",
        "  # Check dimensions of the betas(given by m), and the Sigma covariance matrices\n",
        "  # Initiate the IMCV and MC mean list\n",
        "  mu_IMCV_list = []\n",
        "  mu_MC_list = []\n",
        "  # Initialize the beta-hat-transformation constant. Needed to transform samples generated from EM mixture to beta\n",
        "  beta_hat_operator = scipy.linalg.fractional_matrix_power(X_data_matrix.T @ X_data_matrix, -1/2)\n",
        "  for i in range(T_iterations):\n",
        "    # Initiate X chain, Y chain, and alpha chain\n",
        "    ## X is current state\n",
        "    ## Y is proposed state\n",
        "    ## Alpha chain is the ratio of the pdfs to see if the sample should be accepted: we will also generate uniform RV for acceptance calculation\n",
        "    X_chain = []\n",
        "    Y_chain = []\n",
        "    alpha_chain = []\n",
        "    for j in range(N_samples):\n",
        "      # edge case: X is empty\n",
        "      if len(X_chain) == 0:\n",
        "        # Accept proposed samples from mixed EM Gaussian mixture with probability 1\n",
        "        # Sample from mixture first, and then draw from the chosen distribution\n",
        "        chosen_g = np.random.choice(a = g_j_hat, p = weights)\n",
        "        Y_i = scipy.stats.multivariate_normal.rvs(mean = [0 for _ in range(p)], cov = chosen_g * np.identity(p))\n",
        "        ### Transform sample\n",
        "        Y_i = np.dot(beta_hat_operator, Y_i)\n",
        "        ### Append to X_chain\n",
        "        X_chain.append(np.asarray(Y_i))\n",
        "        ### Append to Y_chain\n",
        "        Y_chain.append(np.asarray(Y_i))\n",
        "        ### Append to Alpha Chain\n",
        "        alpha_chain.append(1)\n",
        "      else:\n",
        "        # Simulate Y_i first and perform transformation\n",
        "        ## Sample g from mixture of gaussians, and then sample from that distribution\n",
        "        chosen_g = np.random.choice(a = g_j_hat, p = weights)\n",
        "        Y_i = scipy.stats.multivariate_normal.rvs(mean = [0 for _ in range(p)], cov = chosen_g * np.identity(p))\n",
        "        Y_i = np.dot(beta_hat_operator, Y_i)\n",
        "        # Append to proposal chain and get current state(X_i)\n",
        "        Y_chain.append(np.asarray(Y_i))\n",
        "        X_i = X_chain[-1]\n",
        "        # Calculate pseudo-prior for acceptance/rejection sampling\n",
        "        ## Note that I sample the g-priors first, to ensure that both the pseudo-prior for X_i and Y_i are calculated from the same sets of g-priors\n",
        "        g_priors_for_pseudo_prior = scipy.stats.lomax(c = 1).rvs(size = L)\n",
        "        alpha_i_numerator = (linear_regression_pseudo_prior_estimation(Y_i, L, X_data_matrix, g_priors_for_pseudo_prior) *\n",
        "                   np.dot(weights, [scipy.stats.multivariate_normal.pdf(x = X_i, mean = [0 for _ in range(p)], cov = g_i * scipy.linalg.inv(X_data_matrix.T @ X_data_matrix)) for g_i in g_j_hat]))\n",
        "        alpha_i_denominator = (linear_regression_pseudo_prior_estimation(X_i, L, X_data_matrix, g_priors_for_pseudo_prior) *\n",
        "                   np.dot(weights, [scipy.stats.multivariate_normal.pdf(x = Y_i, mean = [0 for _ in range(p)], cov = g_i * scipy.linalg.inv(X_data_matrix.T @ X_data_matrix)) for g_i in g_j_hat]))\n",
        "        alpha_i = min(1, alpha_i_numerator/alpha_i_denominator)\n",
        "        alpha_chain.append(alpha_i)\n",
        "        # acceptance rejection\n",
        "        U = np.random.uniform()\n",
        "        if U < alpha_i:\n",
        "          X_chain.append(np.asarray(Y_i))\n",
        "        else:\n",
        "          X_chain.append(np.asarray(X_i))\n",
        "    # Calculate the estimator means\n",
        "    ## Returning the same values for all of the terms here. Big issue with this function. Need some dependence of F function (marginal likelihood of model) on current states\n",
        "    ## Y and X\n",
        "    mu_MC_i = np.mean([conditional_y_on_beta(X_chain[i], X_data_matrix, Y_response_vector, sigma_error) for i in range(len(X_chain))])\n",
        "    mu_MC_list.append(float(mu_MC_i))\n",
        "    ### Perhaps we induce dependence on the current states of the chains (X,Y) by sampling sigma (like a Gibbs sampler)?\n",
        "    mu_IMCV_i = np.mean([conditional_y_on_beta(X_chain[i], X_data_matrix, Y_response_vector, sigma_error) +\n",
        "        alpha_chain[i] * (conditional_y_on_beta(Y_chain[i], X_data_matrix, Y_response_vector, sigma_error) - conditional_y_on_beta(X_chain[i], X_data_matrix, Y_response_vector, sigma_error))\n",
        "        - (conditional_y_on_beta(Y_chain[i], X_data_matrix, Y_response_vector, sigma_error) - expectation_f_function_y_likelihood(weights, g_j_hat, X_data_matrix, Y_response_vector, sigma_error))\n",
        "    for i in range(len(Y_chain))])\n",
        "    mu_IMCV_list.append(float(mu_IMCV_i))\n",
        "  return mu_MC_list, mu_IMCV_list"
      ],
      "metadata": {
        "id": "bQaRVG5fkg3w"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check here\n",
        "sample = X_data_matrix_from_m(X, m = \"123\")\n",
        "\n",
        "m = \"123\"\n",
        "print([0 for _ in range(len(m))])\n",
        "chosen_g = np.random.choice(a = np.array(no_update_mu_diagonal_sigma[4]), p = np.array(no_update_mu_diagonal_sigma[0]))\n",
        "print(no_update_mu_diagonal_sigma[4] @ no_update_mu_diagonal_sigma[0])\n",
        "print([float(scipy.stats.invgamma.rvs(a = 1/2, scale = 1/2)) for i in range(2)])\n",
        "sample.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPN5_38vkj8U",
        "outputId": "e4d5e39e-6497-441f-ac13-70a8b1917b6d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0]\n",
            "7.115488454353405\n",
            "[2.8966020021725547, 160.53845704638007]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuing the m = \"123\" example as a sample test to see if our code works -- trying to recover the numbers from the paper\n",
        "test_run = multivariate_linear_regression_metropolis(4, 50, 50, np.asarray(no_update_mu_diagonal_sigma[0]).ravel(), np.asarray(no_update_mu_diagonal_sigma[4]).ravel(),\n",
        "                                                     \"123\", scipy.stats.lomax(c=1), X_data_matrix_from_m(X, m = \"123\"), y, 100, 2.5)"
      ],
      "metadata": {
        "id": "KffH_evXkl8A"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu_IMCV_test = test_run[1]\n",
        "mu_IMCV_test_log10 = [-math.log10(mu_IMCV_test[i]) for i in range(len(mu_IMCV_test))]\n",
        "print(np.mean([-math.log10(mu_IMCV_test[i]) for i in range(len(mu_IMCV_test))]))\n",
        "print(np.mean([-math.log10(mu_MC_test[i]) for i in range(len(mu_MC_test))]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHEnMpmoknpp",
        "outputId": "a52415e0-bc01-41f0-8586-06e20f30b672"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76.77042445667193\n",
            "76.58709750541276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the CMC estimator, we actually don't use the Gibbs sampler. Instead, we sample directly from $\\pi(x)$. So below I will create a symmetric Metropolis sampler to sample dependent samples from $\\pi(x)$"
      ],
      "metadata": {
        "id": "cwCw4Zz28Q2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def symmetric_metro_betas(y, X_data_matrix, T_iterations, n_samples, F_function,  g_prior, m, burn_in):\n",
        "  # Check to see if pi_distribution, and g_prior can be sampled from\n",
        "  for name, dist in [(\"g_prior\", g_prior)]:\n",
        "    if not all(hasattr(dist, attr) for attr in [\"rvs\", \"pdf\", \"expect\"]):\n",
        "        raise ValueError(\n",
        "            f\"{name} must be a valid frozen scipy.stats distribution with .rvs(), .pdf(), and .expect(). Got: {type(dist)}\"\n",
        "        )\n",
        "  mu_MC = []\n",
        "  # Calculate dimension of model -- needed for sampling of beta from multivariate normal\n",
        "  n = len(m)\n",
        "  # Calculate inverse quad of X data matrix\n",
        "  X_inverse = np.linalg.inv(X_data_matrix.T @ X_data_matrix)\n",
        "  for i in range(T_iterations):\n",
        "    X_chain = []\n",
        "    for j in range(n_samples):\n",
        "      if len(X_chain) == 0:\n",
        "        # Accept with probability of 1\n",
        "        ## Sample g prior first\n",
        "        g_i = float(g_prior.rvs())\n",
        "        ## Then sample beta\n",
        "        Y_i = scipy.stats.multivariate_normal.rvs(mean = [0 for _ in range(n)],\n",
        "                                                cov = g_i * X_inverse, size = 1)\n",
        "        X_chain.append(Y_i)\n",
        "      else:\n",
        "        X_i = X_chain[-1]\n",
        "        g_i = float(g_prior.rvs())\n",
        "        Y_i = scipy.stats.multivariate_normal.rvs(mean = [0 for _ in range(n)],\n",
        "                                              cov = g_i * X_inverse, size = 1)\n",
        "        # Acceptance-rejection\n",
        "        acceptance_denominator = scipy.stats.multivariate_normal.logpdf(X_i, mean = [0 for _ in range(n)], cov = g_i * X_inverse)\n",
        "        acceptance_numerator = scipy.stats.multivariate_normal.logpdf(Y_i, mean = [0 for _ in range(n)], cov = g_i * X_inverse)\n",
        "        log_diff = acceptance_numerator - acceptance_denominator\n",
        "        U = float(np.random.uniform(0, 1))\n",
        "        ## Adding some numerical stability fixes to the acceptance/rejection process\n",
        "        if log_diff > 0:\n",
        "          # Accept with probability of 1\n",
        "          ## If log_diff > 0, then min(1, fraction of acceptance) becomes 1\n",
        "          X_chain.append(Y_i)\n",
        "        else:\n",
        "          # Calculate the acceptance/rejection probability\n",
        "          alpha_i = math.exp(log_diff)\n",
        "          if U < alpha_i:\n",
        "            X_chain.append(Y_i)\n",
        "          else:\n",
        "            X_chain.append(X_i)\n",
        "    # Toss out burn_in_samples\n",
        "    X_chain = X_chain[burn_in:]\n",
        "    mu_MC_i = np.mean([F_function(X_chain[i]) for i in range(len(X_chain))])\n",
        "    mu_MC.append(float(mu_MC_i))\n",
        "  return mu_MC"
      ],
      "metadata": {
        "id": "AKARai4V8dp3"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_run_MC = symmetric_metro_betas(np.asarray(y), X_data_matrix_from_m(X, m = \"123\"), 50, 50,\n",
        "                                    F_function = lambda x: conditional_y_on_beta(x, X_data_matrix_from_m(X, m = \"123\"), np.asarray(y), 2.5),\n",
        "                                    g_prior = scipy.stats.lomax(c=1), m = \"123\", burn_in = 0)"
      ],
      "metadata": {
        "id": "_GolSdlyCOtI"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_run_MC_log10 = [-math.log10(test_run_MC[i]) for i in range(len(test_run_MC))]\n",
        "print(np.mean(test_run_MC_log10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WCQ39fMGAt3",
        "outputId": "5e72e0a2-be12-4ca5-d2e1-decafdf7dc9a"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.72014579387807\n",
            "106677.538432929\n"
          ]
        }
      ]
    }
  ]
}