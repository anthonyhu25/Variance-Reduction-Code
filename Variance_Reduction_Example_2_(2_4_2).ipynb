{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQjOQ8djhD+DhWxUMQ468S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyhu25/Variance-Reduction-Code/blob/main/Variance_Reduction_Example_2_(2_4_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bBwmgV2gexDp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import linalg\n",
        "import math\n",
        "import scipy\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import rv_continuous, rv_discrete\n",
        "from scipy.stats._distn_infrastructure import rv_frozen\n",
        "from scipy.special import logsumexp\n",
        "import scipy.integrate\n",
        "import warnings\n",
        "import sys\n",
        "import statistics\n",
        "import pandas as pd\n",
        "from IPython.display import display, Math, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Bayesian model selection in non-conjugate linear regression"
      ],
      "metadata": {
        "id": "Y57cOJC_htca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider $y = XŒ≤ + œµ$, where $y = (y_{1} ,... , y_{N})$ a column vector of observations, $X$ is an $N \\times p$ design matrix, $Œ≤ = (\\beta_{1},...,\\beta_{p})$ is a column vector of regression coefficients, and $œµ \\sim N(0, \\sigma^2 I_{n})$ is the error vector.\n",
        "\n",
        "For Bayesian model selection, we seek to calculate posterior model probabilities $f(m|y) \\propto f(m)f(y|m)$ for every $m$ specified by $Œ≤_{m}$, where we arrange $p$ elements from $Œ≤$ with $2^p$ possible sets (the power set) for each design matrix $X_{m}$. So, we need to calculate the marginal likelihood of y with respect to model m, given by $f(y|m) = ‚à´f(y|m, \\beta_{m})f(\\beta_{m}|m)dŒ≤_{m}$, such that $f(y|m, \\beta_{m})$ is the conditional likelihood function of y on $m$ and $\\beta_{m}$, and $f(\\beta_{m}|m)$ is the prior density of model $m$.\n",
        "\n",
        "In this example, we seek to estimate the density of $f(y|m, Œ≤_{m})$ analytically. So, we are implicitly finding the expectation of $f(y|m, \\beta_{m})$, or $E_{q}(f(y|m, \\beta_{m})$ -- in this example, we set the function $F(x) = f(y|m, \\beta_{m})$. To do so, note that we have two priors in this example: one on $g$ and one on $\\beta_{m}|g, m$. Let N = 50 datapoints, with predictors $X_{i} \\sim N(0,1), i = 1,...,4$ and response $Y \\sim N(4X_{3} + 4X_{4}, 2.5 ^ 2)$. Suppose we choose a proposal density for $\\beta_{m}$ in the form\n",
        "\n",
        "$$\n",
        "q(\\beta_{m}) = \\sum_{i=1}^{K} w_{i}N(\\beta_{m}|0, g_{i}(X_{m}^{\\top}X_{m})^{-1})\n",
        "$$\n",
        "\n",
        "such that $\\sum_{i=1}^{K} w_{i} = 1, 0 < w_{i} < 1$\n",
        "\n",
        "Note that the g-prior is defined as $p(g) = (1+g)^{-2}, g > 0$ in this example."
      ],
      "metadata": {
        "id": "DR-d_TYRhtmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will simulate the g-prior by using the Lomax distribution directly from [SciPy](https://scipy.github.io/devdocs/reference/generated/scipy.stats.lomax.html), given below as:\n",
        "\n",
        "$f(x|c) = \\frac{c}{(1+x)^{c+1}}$\n",
        "\n",
        "So the g-prior is actually a Lomax distribution with shape parameter($c$) equal to 1.\n"
      ],
      "metadata": {
        "id": "qOqUrRwBh6b8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The numbers for this example will be from the $X_{123}$ model, which means that the first three covariates $(1,2,3)$, denoted as $m$ in the paper, will be used in the model. However, I will run all of the possible covariates combinations for comparison of the algorithm."
      ],
      "metadata": {
        "id": "r67r8R4oh2Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the X and Y matrices\n",
        "## N and p are set from the paper\n",
        "N = 50\n",
        "p = 4\n",
        "# Generate X first\n",
        "mu = 0\n",
        "sigma = 1\n",
        "X_1 = np.random.normal(mu, sigma, size = N)\n",
        "X_2 = np.random.normal(mu, sigma, size = N)\n",
        "X_3 = np.random.normal(mu, sigma, size = N)\n",
        "X_4 = np.random.normal(mu, sigma, size = N)\n",
        "X = np.column_stack((X_1, X_2, X_3, X_4))\n",
        "# Generate Y, note the distribution of Y is defined above\n",
        "y = []\n",
        "for i in range(len(X_1)):\n",
        "  Y_i = np.random.normal(loc = 4 * X_3[i] + 4 * X_4[i], scale = 2.5)\n",
        "  y.append(Y_i)"
      ],
      "metadata": {
        "id": "8SH8cAMfh1zD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beta_generation(N, g_prior_function, m):\n",
        "  betas = []\n",
        "  g = []\n",
        "  for i in range(N):\n",
        "    # If prior function is a scipy frozen RV. can directly sample from its RVs attribute call\n",
        "    if isinstance(g_prior_function, rv_frozen):\n",
        "      # Generate g prior first\n",
        "      g_i = g_prior_function.rvs(size = 1)\n",
        "      # Then generate beats\n",
        "      beta_i = np.random.multivariate_normal(mean = [0 for i in range(len(m))],\n",
        "                                           cov = g_i * np.identity(len(m)))\n",
        "      betas.append(beta_i.tolist())\n",
        "      g.append(g_i)\n",
        "    else:\n",
        "    # Otherwise, use a custom g prior function (used for clipping), and have to use correct function calls for that\n",
        "      g_i = g_prior_function(size = 1)\n",
        "      beta_i = np.random.multivariate_normal(mean = [0 for i in range(len(m))],\n",
        "                                           cov = g_i * np.identity(len(m)))\n",
        "      betas.append(beta_i.tolist())\n",
        "      g.append(g_i)\n",
        "    # sample beta from multivariate normal, with 0 mean and gI(m) covariance matrix\n",
        "\n",
        "  # 1st element is the simulated betas, 2nd element is the simulated g values\n",
        "  return betas, g"
      ],
      "metadata": {
        "id": "dFskKbIyh104"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that m is given as a string in this argument. This matches with the notation in the paper\n",
        "# if m = \"123\", then variables 1, 2, and 3 are chosen as the data matrix\n",
        "m = \"123\"\n",
        "A = beta_generation(1000, scipy.stats.lomax(c=1), m)\n",
        "\n",
        "# In this example, K(number of Gaussians to generate conditional distribution of Beta_m) is equal to 4\n",
        "K = 4"
      ],
      "metadata": {
        "id": "nltHSxxAiX5-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that we need further add assumptions to recover the $\\hat\\Sigma_{j}= \\hat g_{i}I_{(m)}$ covariance structure in the EM algorithm. Although I can see that the $\\hat\\Sigma_{j}$ covariance matrices have similar diagonal elements and relatively small non-diagonal values, I still cannot infer what $\\hat g_{i}$ is from each $\\hat \\Sigma_{j}$. We can still sample from the covariance matrices $\\Sigma_{j}$, but the proposal density would now take the form of $q(\\hatŒ≤) = \\sum_{j=1}^{K} \\hat w_{j}N(\\hatŒ≤|0, \\hat\\Sigma_{j})$, and perform the same linear transformation $\\beta_{m} = (X_{m}^{T}X_{m})^{-\\frac{1}{2}}\\hat Œ≤$ to obtain the estimates of the coefficients.\n",
        "\n",
        "Here, I will derive the necessary steps for the M-step of this algorithm, from the joint log-likelihood of the EM algorithm, in order to enforce the $\\hat g_{i}I_{(m)}$ structure.\n",
        "\n",
        "Let $ùìõ(g_{j})$ be the joint log-likelihood of the datapoints, conditional on the fact that the covariance matrix for the Gaussian is $g_{j}I_{(m)}$.\n",
        "\n",
        "$$ùìõ(g_{j}) = \\sum_{i=1}^{N}r_{ij} log N(Œ≤_{i}|\\mu_{j} = 0_{m}, g_{j}I_{(m)})$$\n",
        "\n",
        "$$= \\sum_{i=1}^{N} r_{ij} log[(2œÄ)^{-m/2} |g_{j}I_{m}|^{-1/2}exp(-\\frac{1}{2}\\beta_{i}^{T}(g_{j}I_{m})^{-1}\\beta_{i})]$$\n",
        "\n",
        "$$ = \\sum_{i=1}^{N} r_{ij} [-\\frac{m}{2}log(2\\pi g_{j}) - \\frac{1}{2g_{j}}\\beta_{i}^{T}\\beta_{i}]$$\n",
        "\n",
        "$$ = -\\frac{m}{2}log(2\\pi g_{j})(\\sum_{i=1}^{N} r_{ij}) - \\frac{1}{2g_{j}}\\sum_{i=1}^{N}r_{ij}||\\beta_{i}||^{2}$$\n",
        "\n",
        "Note that the term $|g_{j}I_{m}|$ is equivalent to $g_{j}^{m}|I_{m}|$, due to the property of a determinant with a scalar of a matrix. Hence, the term $log[(2\\pi)^{-\\frac{m}{2}}|g_{j}I_{m}|^{-\\frac{1}{2}}]$ can be simplified to $-\\frac{m}{2}log(2\\pi g_{j})$\n",
        "\n",
        "Take derivative with respect to $g_{j}$ and set to $0$ to solve for $\\hat g_{j}$\n",
        "\n",
        "$$\\frac{dùìõ(g_{j})}{dg_{j}} = -\\frac{m}{2g_{j}}\\sum_{i=1}^{N}r_{ij} + \\frac{1}{2g_{j}^{2}}\\sum_{i=1}^{N}r_{ij}||Œ≤_{i}||^{2} = 0$$\n",
        "\n",
        "$$ -mg_{j}\\sum_{i=1}^{N}r_{ij} + \\sum_{i=1}^{N}r_{ij}||Œ≤_{i}||^{2} = 0$$\n",
        "\n",
        "$$\\hat g_{j} = \\frac{\\sum_{i=1}^{N}r_{ij}||Œ≤_{i}||^{2}}{m\\sum_{i=1}^{N}r_{ij}}$$\n",
        "\n",
        "Now, instead of updating $\\Sigma_{j}$, we use this to update $g_{j}$ and maintain the diagonal structure of the $\\Sigma_{j}$, as it was presented in the paper.\n",
        "\n",
        "I will write a new EM algorithm below for the change. Note that I will also add some quality of life changes (sometimes depending on the values drawn from the priors I can have extremely small values for $g_{i}$ or $N(0, gI_{m})$. So I will use the log trick(adding log to the expression and then exponentiate to cancel out the log) for numerical stability purposes."
      ],
      "metadata": {
        "id": "-TLbvgWZicRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def EM_algorithm_Gaussians_diagonal_matrix(K, list_betas, p, g_sampler, N_iterations, update_mu, only_print_final = False,\n",
        "                           init_means=None, init_covs=None):\n",
        "    for i in range(len(list_betas)):\n",
        "        if len(list_betas[i]) != p:\n",
        "            warnings.warn(\"Dimensions of betas and p do not match.\")\n",
        "            sys.exit(1)\n",
        "    w_i = np.random.dirichlet(np.ones(K)).tolist()\n",
        "    initial_w_i = w_i.copy()\n",
        "    g_i = [0 for _ in range(K)]\n",
        "    # Log Likelihood list for convergence diagnostics -- should always be nondecreasing\n",
        "    log_likelihoods_list = []\n",
        "    if init_means is not None and len(init_means) == K:\n",
        "        mu_list = init_means\n",
        "    elif init_means is not None and len(init_means) != K:\n",
        "        warnings.warn(\"Number of means and K do not match.\")\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        mu_list = [[0 for _ in range(p)] for _ in range(K)]\n",
        "    if init_covs is not None:\n",
        "        sigma_list = init_covs\n",
        "    elif hasattr(g_sampler, \"rvs\"):\n",
        "        g_i = g_sampler.rvs(size = K)\n",
        "        sigma_list = [g * np.identity(p) for g in g_i]\n",
        "    else:\n",
        "        g_i = g_sampler(K)\n",
        "        sigma_list = [g * np.identity(p) for g in g_i]\n",
        "    # Initialize the initial g_i for comparison of algorithm\n",
        "    initial_g_i = g_i.copy()\n",
        "    for v in range(N_iterations):\n",
        "      ## Adding total log_likelihood counter for measure of model performance\n",
        "        ll_total = 0.0\n",
        "        responsibilities = np.empty(shape=(len(list_betas), K))\n",
        "        # E-step\n",
        "        for i in range(len(list_betas)):\n",
        "          # initiate beta\n",
        "            beta_i = list_betas[i]\n",
        "          # calculate likelihoods\n",
        "          ## using log likelihood for numerical stability purposes\n",
        "          ### Indeterminate values when calculating EM, especially when g is small, due to abnormally large or small PDFs, which can cause 0-near or\n",
        "          ### super-large denominators. This in turn causes NAN values, which causes issues with the algorithm\n",
        "            log_likelihoods = [\n",
        "                math.log(w_i[j]) + scipy.stats.multivariate_normal.logpdf(\n",
        "                    beta_i, mean=mu_list[j], cov=sigma_list[j], allow_singular=True\n",
        "                )\n",
        "                for j in range(K)\n",
        "            ]\n",
        "            denom = scipy.special.logsumexp(log_likelihoods)\n",
        "            ll_total += denom\n",
        "            for j in range(K):\n",
        "              # Exponentiate to remove the log.\n",
        "              responsibilities[i][j] = math.exp(log_likelihoods[j] - denom)\n",
        "        # M-step\n",
        "        for i in range(K):\n",
        "            r_i = responsibilities[:, i]\n",
        "            total_r_i = np.sum(r_i)\n",
        "            if update_mu == True:\n",
        "              # Calculate Mus\n",
        "              mu_new = np.sum([\n",
        "                  r_i[j] * np.array(list_betas[j]) for j in range(len(list_betas))\n",
        "              ], axis=0) / total_r_i\n",
        "              mu_list[i] = mu_new\n",
        "              # Calculate g_is based on new mus, and previously outlined algorithm\n",
        "              g_hat_new = np.sum([r_i[j] * np.linalg.norm(np.array(list_betas[j]) - mu_new) ** 2 for j in range(len(list_betas))]) / (p * total_r_i)\n",
        "              g_i[i] = g_hat_new\n",
        "              # recalculate sigmas\n",
        "              sigma_list[i] = g_hat_new * np.identity(p)\n",
        "            else:\n",
        "              # No update mu -- just calculate sigmas and g_is\n",
        "              g_hat_new = np.sum([r_i[j] * np.linalg.norm(np.array(list_betas[j])) ** 2 for j in range(len(list_betas))]) / (p * total_r_i)\n",
        "              g_i[i] = g_hat_new\n",
        "              sigma_list[i] = g_hat_new * np.identity(p)\n",
        "        # Recalculate weights\n",
        "        w_i = [float(np.sum(responsibilities[:, i]) / len(list_betas)) for i in range(K)]\n",
        "        if only_print_final == False:\n",
        "          # print weights at each iteration of process\n",
        "          print(f\"Iteration {v+1}: Weights: {w_i}: Log Likelihood: {ll_total}\")\n",
        "          log_likelihoods_list.append(np.float64(ll_total))\n",
        "        else:\n",
        "          # Print only last step\n",
        "          if v == N_iterations - 1:\n",
        "            print(f\"Iteration {v+1}: Final Weights: {w_i}: Log Likelihood : {ll_total}\")\n",
        "            log_likelihoods_list.append(np.float64(ll_total))\n",
        "    # Return final weights, initial weights, list of vector of mu, initial g_i (sampled from g-prior),\n",
        "    ## Final g_prior calculated from EM, and log-likelihood list (for convergence and debugging purposes)\n",
        "    return w_i, initial_w_i, mu_list, initial_g_i, g_i, log_likelihoods_list"
      ],
      "metadata": {
        "id": "LfQ-msOoifF0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_update_mu_diagonal_sigma = EM_algorithm_Gaussians_diagonal_matrix(K, A[0], len(m), scipy.stats.lomax(c=1), 20, False)\n",
        "print(f\"Final weights:{no_update_mu_diagonal_sigma[0]}\")\n",
        "print(f\"Initial weights: {no_update_mu_diagonal_sigma[1]}\")\n",
        "print(f\"Final mean vectors: {no_update_mu_diagonal_sigma[2]}\")\n",
        "print(f\"initial g_j: {no_update_mu_diagonal_sigma[3]}\")\n",
        "print(f\"final g_j: {no_update_mu_diagonal_sigma[4]}\")\n",
        "# Plot the log likelihood as it is calculated through iterations\n",
        "plt.plot(no_update_mu_diagonal_sigma[5])\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Log Likelihood\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "7wZx56LXihuM",
        "outputId": "5daad41c-62f6-44fe-d490-dbd11aa64f37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Weights: [0.07744434344898628, 0.16118053325623813, 0.7031722207407278, 0.05820290255404771]: Log Likelihood: -6452.729184558025\n",
            "Iteration 2: Weights: [0.1347728499232689, 0.14335757270852525, 0.6338964840215722, 0.08797309334663364]: Log Likelihood: -5355.199088480468\n",
            "Iteration 3: Weights: [0.15998529596204517, 0.11934281419285535, 0.5999767232340855, 0.12069516661101412]: Log Likelihood: -5296.439339441441\n",
            "Iteration 4: Weights: [0.17131823871430668, 0.09671108358209511, 0.579262261930126, 0.15270841577347222]: Log Likelihood: -5269.525966432553\n",
            "Iteration 5: Weights: [0.17934771806881633, 0.07775594695147717, 0.5620378246217154, 0.1808585103579913]: Log Likelihood: -5248.882944612376\n",
            "Iteration 6: Weights: [0.18599125663768376, 0.0631376510482116, 0.5478535407403187, 0.20301755157378598]: Log Likelihood: -5233.340466108252\n",
            "Iteration 7: Weights: [0.19165243643456445, 0.05263756964663357, 0.5370643330150652, 0.21864566090373677]: Log Likelihood: -5222.969828276537\n",
            "Iteration 8: Weights: [0.19655680450748883, 0.04543252450313617, 0.5294929001720406, 0.22851777081733435]: Log Likelihood: -5216.771727024137\n",
            "Iteration 9: Weights: [0.20089663298277485, 0.04060139414052897, 0.5245572034873782, 0.23394476938931796]: Log Likelihood: -5213.298338060623\n",
            "Iteration 10: Weights: [0.20482668286094163, 0.037397893747729345, 0.5215441542408632, 0.23623126915046577]: Log Likelihood: -5211.362512572211\n",
            "Iteration 11: Weights: [0.20845915852701902, 0.03527211108951269, 0.519817824557865, 0.2364509058256033]: Log Likelihood: -5210.2131516782465\n",
            "Iteration 12: Weights: [0.2118677676218253, 0.03383657473842402, 0.5189039350473025, 0.23539172259244812]: Log Likelihood: -5209.440345862222\n",
            "Iteration 13: Weights: [0.21509675081781743, 0.03283228633321609, 0.5184834824526603, 0.2335874803963062]: Log Likelihood: -5208.845913160371\n",
            "Iteration 14: Weights: [0.2181706332976359, 0.03209426725697111, 0.5183545564124923, 0.23138054303290084]: Log Likelihood: -5208.343439592188\n",
            "Iteration 15: Weights: [0.22110204887936508, 0.03152085445643958, 0.5183942609206801, 0.2289828357435153]: Log Likelihood: -5207.897810143082\n",
            "Iteration 16: Weights: [0.223897072144334, 0.031050603316792033, 0.5185298261853132, 0.22652249835356095]: Log Likelihood: -5207.495031742412\n",
            "Iteration 17: Weights: [0.22655853837247664, 0.03064674823054564, 0.51871903433584, 0.22407567906113762]: Log Likelihood: -5207.129120045232\n",
            "Iteration 18: Weights: [0.22908799402858876, 0.030287381435874044, 0.5189377514886827, 0.2216868730468545]: Log Likelihood: -5206.796907264888\n",
            "Iteration 19: Weights: [0.23148678182945082, 0.029959450175583328, 0.5191722886538734, 0.21938147934109242]: Log Likelihood: -5206.4961215884205\n",
            "Iteration 20: Weights: [0.23375659827512763, 0.029655152368265908, 0.5194148479174071, 0.21717340143919944]: Log Likelihood: -5206.224725457394\n",
            "Final weights:[0.23375659827512763, 0.029655152368265908, 0.5194148479174071, 0.21717340143919944]\n",
            "Initial weights: [0.04921318467103687, 0.008634305615821333, 0.9156329851790669, 0.02651952453407478]\n",
            "Final mean vectors: [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
            "initial g_j: [0.10703062 7.06944176 0.7405646  1.66293524]\n",
            "final g_j: [9.50017658e-02 1.37568134e+02 1.15087502e+00 8.92137692e+00]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Log Likelihood')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are ready to run the Independent Metropolis-Hastings algorithm. Again, note that our proposal density is noted as $q(\\hatŒ≤) = \\sum_{j=1}^{K} \\hat w_{j}N(\\hatŒ≤|0, \\hat g_{i}I_{(m)})$ from the EM algorithm, with estimates $\\{\\hat w_j, \\hat g_j\\}_{j=1}^{K}$. Then, we use the transformation $\\beta_{m} = (X_{m}^{T}X_{m})^{-\\frac{1}{2}}\\hat\\beta_{m}$ to obtain the estimates of the coefficients.\n",
        "\n",
        "We hope that these proposed samples matches the priors given below:\n",
        "\n",
        "$$\\beta_{m}|g, m \\sim N(0, g(X_{m}^{T}X_{m})^{-1}) , p(g) = (1+g)^{-2}, g > 0$$\n",
        "\n",
        "So $\\pi(x)$, the target density in this example, is the conditional prior distribution of $\\beta_{m}$. So for each proposed sample $y$ and current sample $x$ in the MCMC, the acceptance ratio will be:\n",
        "\n",
        "$$\\alpha(x, y) = min (1, \\frac{f_{\\beta_{m}}(y|m)q_{\\beta_{m}}(x)}{f_{\\beta_{m}}(x|m)q_{\\beta_{m}}(y)} ) = min(1, \\frac{f_{\\beta_{m}}(y|m)(X_{m}^{T}X_{m})^{-1}q_{\\hatŒ≤_{m}}(x)}{f_{\\beta_{m}}(x|m)(X_{m}^{T}X_{m})^{-1}q_{\\hat\\beta_{m}}(y)}= min(1, \\frac{f_{\\beta_{m}}(y|m) (X^{T}_{m}X_{m})^{-1/2} \\sum_{j=1}^{K} \\hat w_{j} N(x|0_{m}, \\hat g_{j}I_{(m)} )}{f_{\\beta_{m}}(x|m) (X^{T}_{m}X_{m})^{-1/2} \\sum_{j=1}^{K} \\hat w_{j} N(y|0_{m}, \\hat g_{j}I_{(m}))})$$\n",
        "\n",
        "$$ = min(1, \\frac{f_{\\beta_{m}}(y|m) \\sum_{j=1}^{K} \\hat w_{j} N(x|0_{m}, \\hat g_{j}I_{(m)} )}{f_{\\beta_{m}}(x|m) \\sum_{j=1}^{K} \\hat w_{j} N(y|0_{m}, \\hat g_{j}I_{(m}))})$$\n",
        "\n",
        "2 Things to note:\n",
        "\n",
        "1. For the target density $\\pi(.) = f_{\\beta_{m}}( .|m) = \\int_{0}^{‚àû} f(\\beta_{m}|m, g)p(g)dg = \\int_{0}^{‚àû} N(0, g(X^{T}_{m}X_{m})^{-1}p(g)dg$, which is the integral of the joint product of the two priors given on the support of $g$ (from $0$ to $‚àû$). We can approximate this integral by MCMC, where we take $L$ samples, and approximate the density by averaging $L$ products of the multivariate normal density, and the g-prior density. So, $\\pi(.) ‚âà \\frac{1}{L}\\sum_{i=1}^{L}N(.|0_{m}, g_{i}(X_{m}^{T}X_{m})^{-1}), g_{i} ‚àº p(g) = (1+g)^{-2}$\n",
        "2. We have expressions for the analytical value of $f(y|m)$ and the expectation of the F-function (which is the conditional distribution of y on m and $\\beta_{m}$, or $f(y|m, \\beta_{m})$) $E_{q(\\beta_{m})} (f(y|m, \\beta_{m})$. These  expression are given in the appendix of the paper, as seen [here](https://arxiv.org/pdf/2406.17699#page=52)."
      ],
      "metadata": {
        "id": "284WoCYMjZie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the form given in the paper, we are trying to estimate $F$ function -- the marginal likelihood of $y$, on $m$ : $f(y|m)$ given by the integral:\n",
        "\n",
        "$$f(y|m) = ‚à´f(y|m, Œ≤_{m})f(Œ≤_{m}|m)dŒ≤_{m} = ‚à´f(y|m, Œ≤_{m})[N(0, g(X_{m}^{T}X_{m})^{-1}p(g)dg]dŒ≤_{m} = ‚à´(2 \\pi œÉ^{2})^{-\\frac{N}{2}}exp[-\\frac{1}{2œÉ_{2}}‚àë_{i=1}^{N}(y_{i} - x_{i}^{T}Œ≤_{m}) ^{2}]N(0, g(X_{m}^{T}X_{m})^{-1})p(g)d\\beta_{m}dg = \\\\ C\\int p(g)[\\int (2\\pi g)^{-\\frac{d}{2}}|(X_{m}^{T}X_{m})^{-1}|^{-\\frac{1}{2}}exp[\\frac{1}{œÉ_{2}} \\sum_{i=1}^{N}y_{i}x_{i}^{T}Œ≤_{m}- \\frac{1}{2}(\\frac{1}{œÉ^{2}}+\\frac{1}{g})Œ≤_{m}^{T}(X_{m}^{T}X_{m})Œ≤_{m}]dŒ≤_{m}]dg\n",
        "= \\\\ C‚à´(1+\\frac{g}{œÉ^{2}})^{-\\frac{d}{2}}exp[\\frac{1}{2 œÉ^{2}}\\frac{g}{g+œÉ^{2}}(\\sum_{i=1}^{N}y_{i}x_{i}^{T}(X_{m}^{T}X_{m})^{-1}(\\sum_{i=1}^{N}y_{i}x_{i}^{T})^{T}](1+g)^{-2}dg =\n",
        "\\\\C‚à´(1+\\frac{g}{\\sigma^{2}})^{-\\frac{d}{2}}exp[\\frac{1}{2 \\sigma^{2}} \\frac{g}{g + \\sigma^{2}}(yX_{m})(X_{m}^{T}X_{m})^{-1}(yX_{m})^{T}](1+g)^{-2}dg $$\n",
        "\n",
        "for $i = 1,...,N$ observations corresponding to $(x_{i}, y_{i})$\n",
        "\n",
        "Where $C = (2\\pi œÉ^{2})^{-\\frac{N}{2}}exp[-\\frac{1}{2œÉ^{2}} \\sum_{i=1}^{N} y_{i}^{2}]$, and $x_{i}$ is the $i$th row of data matrix $X_{m}$.\n",
        "\n",
        "Since this is a univariate integral, we can explicitly calculate the marginal likelihood\n",
        "\n",
        "With the proposed variance reduction scheme, we can approximate this integral, with $N$ samples, by calculating its expectation with regards to the target density $\\pi(v)$, along with reducing the sampler's variance, by using the sum given from integrating the marginal likelihood $f(y|m, Œ≤_{m}$) with respect to the proposal density $q(Œ≤_{m})$, with the full results given [here](https://arxiv.org/pdf/2406.17699#page=52):\n",
        "\n",
        "$$ùîº_{q(Œ≤_{m})}(f(y|m, Œ≤_{m})) = \\int f(y|m, Œ≤_{m})q(Œ≤_{m})dŒ≤_{m} \\\\\n",
        " = C \\sum_{j=1}^{K} w_{j}(1+\\frac{g_{j}}{œÉ^{2}})^{-\\frac{p}{2}}exp[\\frac{1}{2œÉ^{2}} \\frac{g_{j}}{g_{j}+œÉ^{2}} (\\sum_{i=1}^{N}y_{i}x_{i})^{T} (X_{m}^{T}X_{m})^{-1}(\\sum_{i=1}^{N}y_{i}x_{i})^{T}] \\\\\n",
        " = C \\sum_{j=1}^{K}w_{j}(1+\\frac{g_{j}}{\\sigma^{2}})^{-\\frac{p}{2}}exp[\\frac{1}{2\\sigma^{2}}\\frac{g}{g+\\sigma^{2}}(yX_{m})(X_{m}^{T}X_{m})^{-1}(yX_{m})^{T}]$$\n",
        "\n",
        "\n",
        "for $j = 1,...,K$ Gaussians used in the proposal density $q(\\beta_{m})$ and its g-prior $p(g)$, $(x_{i}, y_{i})_{i=1}^{N}$ generated samples for the original data $(X, y)$, $C = (2\\pi œÉ^{2})^{-\\frac{N}{2}}exp[-\\frac{1}{2œÉ^{2}} \\sum_{i=1}^{N} y_{i}^{2}]$ the normalizing constant, $œÉ^{2}$ the scaling factor for the multivariate error term $œµ \\sim N(0, œÉ^{2}I_{m})$, and $p$ is the dimension of the $\\beta_{m}$ or the number of variables selected."
      ],
      "metadata": {
        "id": "yC_UrSCtjbm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike the previous example, we need to create a separate conditional Markov Chain sampling algorithm for Bayesian linear regression.\n",
        "\n",
        "In Bayesian literature, there is a Gibbs-sampling algorithms using g-priors for linear regression on the $g$ prior value wich generates the $\\beta_{m}$ from its conditional prior on $g$ and $m$ (recall that the prior is $\\beta_{m}|g, m \\sim N(0, gI_{m})$\n",
        "\n",
        "Commonly, a prior used for $\\sigma^{2}$ in linear regression is the Jeffrey's prior. The Jeffrey's prior is a non-informative prior used for any parameter space. In [Gelman](https://sites.stat.columbia.edu/gelman/book/BDA3.pdf#page=62), the Jeffrey's prior for a parameter $\\theta$ is formally defined as the square root of the Fisher information $J(\\theta)$ of a distribution:\n",
        "\n",
        "$$p(\\theta) \\propto |J(\\theta)|^{\\frac{1}{2}} = |ùîº((\\frac{dlogP(y|\\theta)}{d\\theta}|Œ∏)^{2})|^{\\frac{1}{2}} = |-ùîº(\\frac{d^{2}log P(y|\\theta)}{d\\theta^{2}} )|^{\\frac{1}{2}}$$\n",
        "\n",
        "where $P(y|\\theta)$ is the density of the response variable $y$, conditional on $\\theta$\n",
        "\n",
        "Sometimes, a Jeffrey's prior for a distribution is an improper distribution, as is the case for the Jeffrey's prior for $\\sigma^{2}$. The Jeffrey's prior on $\\sigma^{2}$ for a Gaussian distribution $p(\\sigma^{2})$ is proportionate to $\\frac{1}{\\sigma^{2}}$, which means that the Jeffrey's prior is a uniform prior on the parameter space of $\\sigma^{2}$. However, the prior $p(\\sigma^{2})$ does not integrate to $1$ over $(0, ‚àû)$. Hence, Jeffrey's prior is an improper distribution. It is also the limit of an inverse gamma\n",
        "\n",
        "Nevertheless, the Jeffrey's prior is commonly used because it is a uniform flat prior on $\\sigma^{2}$, which is represents no prior knowledge of the distribution of $\\sigma^{2}$, and it has a nice conjugate form for the posterior, conditional on $y, X_{m}$, and $g$, which is important because we sample $\\sigma^{2}$ from its posterior, instead of its prior.\n",
        "\n",
        "Using the Jeffrey's prior on $\\sigma^{2}$ ($p(œÉ) \\propto \\frac{1}{\\sigma^{2}}$), we can find the form of the posterior distribution $p(\\sigma^{2}|y, X_{m})$. This posterior distribution is an inverse-Gamma, and its full form listed below, from [Hoff](https://sites.math.rutgers.edu/~zeilberg/EM20/Hoff.pdf#page=162)\n",
        "\n",
        "$$p(\\sigma^{2}|y, X_{m}, g) \\sim inv-Gamma(\\frac{v_{0}+n}{2}, \\frac{v_{0}\\sigma_{0}^{2} + SSR_{g}}{2}) $$\n",
        "\n",
        "Where $SSR_{g} = y^{T}(I - \\frac{g}{g+1}X_{m}(X_{m}^{T}X_{m})^{-1}X^{T})y$.\n",
        "\n",
        "So, with the Gibbs sampler algorithm, we sample $Œ≤$ and $\\sigma^{2}$\n",
        "\n",
        "\n",
        "One more thing note is the paper's construction of $y \\sim N(4X_{3} + 4X_{4}, 2.5^{2})$ implies that the true $Œ≤$ is the vector $(0, 0, 4, 4)$"
      ],
      "metadata": {
        "id": "dYnpOK_Hjd04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f_function_y_likelihood(sigma_error, Y_response_vector, X_data_matrix):\n",
        "  # First to see if dimeinsions match\n",
        "  # number of rows of X_data_matrix and length of Y_response_vector should match\n",
        "  if X_data_matrix.shape[0] != len(Y_response_vector):\n",
        "    warnings.warn(\"Dimensions of X_data_matrix and Y_response_vector do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Calculate constant first using logarithms for numerical stability\n",
        "  ## Find number of samples\n",
        "  N = len(Y_response_vector)\n",
        "  ## Find dimension of Gaussians\n",
        "  d = X_data_matrix.shape[1]\n",
        "  ## Calculate log constant\n",
        "  C = (2 * math.pi * (sigma_error ** 2)) ** (-N/2) * math.exp((-1 / (2 * sigma_error ** 2)) * float(np.sum([y ** 2 for y in Y_response_vector])))\n",
        "  # Numerically integrate\n",
        "  ## Set up function\n",
        "  ### Evaluate the summation of y_i x_i^T using a dot product\n",
        "  yixi = Y_response_vector @ X_data_matrix\n",
        "  ### inverse of X^TX\n",
        "  inv_X = np.linalg.inv(X_data_matrix.T @ X_data_matrix)\n",
        "  ## Time to integrate\n",
        "  ### Write function within the quad() function call using lambda\n",
        "  integral_value, error = scipy.integrate.quad(\n",
        "    lambda g: ((1 + g / (sigma_error ** 2)) ** (-d/2)) * math.exp( (g / (2 * (sigma_error ** 2) * (g + sigma_error ** 2))) * yixi @ inv_X @ yixi.T) * ((1 + g) ** (-2)),\n",
        "    0, np.inf)\n",
        "  # Return the constant times the integral\n",
        "  return C * integral_value"
      ],
      "metadata": {
        "id": "ka4qnFpqjgGt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expectation_f_function_y_likelihood(weights, g_prior_samples, X_data_matrix, Y_response_vector, sigma_error):\n",
        "  # First check to see dimensions match\n",
        "  ## Weights and g_prior should have same dimension -- K\n",
        "  if len(weights) != len(g_prior_samples):\n",
        "    warnings.warn(\"Dimensions of weights and g_prior do not match.\")\n",
        "    sys.exit(1)\n",
        "  ## number of rows of X_data_matrix and length of Y_response_vector should match\n",
        "  if X_data_matrix.shape[0] != len(Y_response_vector):\n",
        "    warnings.warn(\"Dimensions of X_data_matrix and Y_response_vector do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Calculate the constant first\n",
        "  ## Find the number of samples\n",
        "  N = len(Y_response_vector)\n",
        "  ## Find the dimensions of multivariate normals\n",
        "  d = X_data_matrix.shape[1]\n",
        "  ## Calculate the constant\n",
        "  C = (2 * math.pi * (sigma_error ** 2)) ** (- N / 2) * math.exp((-1 / (2 * sigma_error ** 2)) * float(np.sum([y ** 2 for y in Y_response_vector])))\n",
        "  # then sum over the K elements (number of mixtures)\n",
        "  sum = 0\n",
        "  ## Number of distributions used in mixture (K)\n",
        "  K = len(weights)\n",
        "  ## Dimension of the\n",
        "  for i in range(K):\n",
        "    g_i = g_prior_samples[i]\n",
        "    w_i = weights[i]\n",
        "    sum += w_i * (1 + g_i / (sigma_error** 2)) ** (- d / 2) * math.exp((1 / (2 * sigma_error ** 2)) * (g_i / (g_i + sigma_error ** 2)) *\n",
        "                                                                (Y_response_vector @ X_data_matrix) @ # Y_i x_i^T summation\n",
        "                                                                np.linalg.inv(X_data_matrix.T @ X_data_matrix) @ # inverse of product of X^T X\n",
        "                                                                (Y_response_vector @ X_data_matrix).T) # Transpose of Y_i x_i^T summation\n",
        "  return C * sum"
      ],
      "metadata": {
        "id": "E279jfq_jhwy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_pseudo_prior_estimation(z, L, X_data_matrix, g_priors):\n",
        "  # Check dimensions\n",
        "  ## Length of z must be the same as the number of columns of X_data_matrix\n",
        "  if len(z) != X_data_matrix.shape[1]:\n",
        "    warnings.warn(\"Dimensions of z and X_data_matrix do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Solve for (X^TX)^-1 first\n",
        "  ## Need to generate the g-priors first before feeding it through the function\n",
        "  covariance_matrix = np.linalg.inv(X_data_matrix.T @ X_data_matrix)\n",
        "  mean_pdf_calculation = np.mean([scipy.stats.multivariate_normal.pdf(z, mean =  [0 for _ in range(len(z))],\n",
        "                                                  cov = g_priors[i] * covariance_matrix) for i in range(L)])\n",
        "  return float(mean_pdf_calculation)"
      ],
      "metadata": {
        "id": "pfup3FcXjjS7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SSR_g_calculation(g, X, y):\n",
        "  # Calculate \"quadratic\" form of X\n",
        "  quad_X = X @ np.linalg.inv(X.T @ X) @ X.T\n",
        "  return y.T @ (np.eye(quad_X.shape[0]) - (g / (g+1)) * quad_X) @ y"
      ],
      "metadata": {
        "id": "-5le_3Bejk7r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_y_on_beta(beta, X, y, sigma):\n",
        "  # Calculate error first\n",
        "  error = (y - X @ beta).T @ (y - X @ beta)\n",
        "  # Constant\n",
        "  n = X.shape[0]\n",
        "  constant = 1 / math.sqrt( (2 * math.pi * (sigma ** 2)) ** n)\n",
        "  return constant * math.exp( (-1/(2 * (sigma ** 2))) * error)"
      ],
      "metadata": {
        "id": "iDy-ragwjmVW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting columns in X data matrix based on string\n",
        "def X_data_matrix_from_m(X, m):\n",
        "  # Shift one left because of Python indices starting at 0\n",
        "  columns = [(int(i) - 1) for i in m]\n",
        "  return X[:,columns]"
      ],
      "metadata": {
        "id": "Mz8k-k4kjoyY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CMC_gibbs_linear_regression(N_samples, burn_in, X_data_matrix, Y_response_vector, T_iterations, weights_g, g_i_vector):\n",
        "  # Sanity check: X_data_matrix and Y_response_vector dimensions line up\n",
        "  if X_data_matrix.shape[0] != len(Y_response_vector):\n",
        "    warnings.warn(\"Dimensions of X_data_matrix and Y_response_vector do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Define N for the posterior of Sigma -- note it is not the same as the N_samples\n",
        "  N = X_data_matrix.shape[0]\n",
        "  p = X_data_matrix.shape[1]\n",
        "  print((N, p))\n",
        "  mu_MC = []\n",
        "  g_vector = []\n",
        "  # Calculate the quadratic inverse of X^T X using Cholesky once\n",
        "  quad_inverse = np.linalg.inv(X_data_matrix.T @ X_data_matrix)\n",
        "  for i in range(T_iterations):\n",
        "    # First pick and fix from gs from mixture of Gaussians\n",
        "    chosen_g = np.random.choice(a = g_i_vector, p = weights_g)\n",
        "    g_vector.append(chosen_g)\n",
        "    # Also calculate the SSR_g first -- this will be needed for sampling of the sigmas\n",
        "    SSR_g = SSR_g_calculation(chosen_g, X_data_matrix, Y_response_vector)\n",
        "    # Calculate beta_OLS and g_ratio for beta posterior sampling\n",
        "    ## Use quad-inverse, since it is (X^{T} * X) ^{-1} * X^{T} y\n",
        "    beta_OLS = quad_inverse @ X_data_matrix.T @ Y_response_vector\n",
        "    g_ratio = chosen_g / (chosen_g+1)\n",
        "    # Initiate beta,sigma, and likelihood list inside each iteration -- need this for likelihood calculation\n",
        "    beta_list = []\n",
        "    sigma_2_list = []\n",
        "    likelihood_list = []\n",
        "    for j in range(burn_in + N_samples):\n",
        "      # generate the sigma first\n",
        "      sigma_2_iteration = scipy.stats.invgamma.rvs(a = N/2, scale = SSR_g/2, size = 1)\n",
        "      sigma_2_list.append(sigma_2_iteration)\n",
        "      # generate beta conditional on sigma\n",
        "      ## using the quad-inverse again for the covariance, since it is g * sigma^{2} * (X^T * X)^{-1}\n",
        "      beta_iteration = scipy.stats.multivariate_normal.rvs(mean = g_ratio * np.asarray([0 for _ in range(p)]),\n",
        "                                                           cov = g_ratio * sigma_2_iteration * quad_inverse,\n",
        "                                                           size = 1)\n",
        "      beta_list.append(beta_iteration)\n",
        "      # Calculate joint likelihood of data, conditional on each sigma and beta\n",
        "      ## Directly calculate using the PDF of multivariate normal\n",
        "      likelihood_iteration = scipy.stats.multivariate_normal.pdf(Y_response_vector - X_data_matrix @ beta_iteration.T, mean = [0 for _ in range(N)],\n",
        "                                                                 cov = sigma_2_iteration * np.eye(N))\n",
        "      likelihood_list.append(float(likelihood_iteration))\n",
        "    # Throw away burn_in_samples\n",
        "    beta_list = beta_list[burn_in:]\n",
        "    sigma_2_list = sigma_2_list[burn_in:]\n",
        "    likelihood_list = likelihood_list[burn_in:]\n",
        "    # Calculate the estimator means\n",
        "    mu_MC_i = np.mean(likelihood_list)\n",
        "    mu_MC.append(float(mu_MC_i))\n",
        "  return mu_MC, g_vector"
      ],
      "metadata": {
        "id": "_7pk0Rcvjqeo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test = CMC_gibbs_linear_regression(1, 0, X_data_matrix_from_m(X, \"134\"), np.asarray(y), 1000, np.asarray(no_update_mu_diagonal_sigma[0]), np.asarray(no_update_mu_diagonal_sigma[4]))\n",
        "mu_MC_test = test[0]\n",
        "mu_MC_test_log10 = [-math.log10(mu_MC_test[i]) for i in range(len(mu_MC_test))]\n",
        "print(np.mean(mu_MC_test_log10))\n",
        "print(statistics.variance(mu_MC_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgPsD6_ujsZX",
        "outputId": "146c95d0-dea8-4385-e8f0-1f3d29f37589"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 3)\n",
            "80.99970781520547\n",
            "3.67722188333767e-140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(-math.log10(f_function_y_likelihood(2.5, np.asarray(y), X_data_matrix_from_m(X, \"123\"))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_BYWlrRjuZF",
        "outputId": "0fb85dbb-371f-4184-da43-aa82f8d8fd8b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87.11818043441309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are ready to initiate the Metropolis-Hastings algorithm for variance reduction. Again, note that the $F$ function is the marginal likelihood of response variable $y$, given the predictors chosen (as denoted as $m$). Hence, $$F(y) = f(y|\\beta_{m},X, œÉ) = (2\\pi\\sigma^{2})^{-\\frac{n}{2}}exp(-\\frac{(y-X\\beta_{m})^{T}(y-X\\beta_{m})}{2\\sigma^{2}})$$\n",
        "\n",
        "$$Œº_{IMCV} = \\frac{1}{n}\\sum_{i=1}^{n}{F(X_{i}) + Œ±(X_{i}, Y_{i})(F(Y_{i})-F(X_{i})) - (F(Y_{i}) - ùîº_{q}(F(Y_{i})))}$$"
      ],
      "metadata": {
        "id": "hB6E13Ypjxyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multivariate_linear_regression_metropolis(K, N_samples, T_iterations,\n",
        "                                              weights, g_j_hat, m, g_prior_function, X_data_matrix, Y_response_vector,\n",
        "                                              L, sigma_error):\n",
        "  # Sanity checks: make sure K, length of weights, and length of g_j_hat is the same\n",
        "  if len(weights) != len(g_j_hat) or len(weights) != K:\n",
        "    warnings.warn(\"Dimensions of weights, g_i, and K do not match.\")\n",
        "    sys.exit(1)\n",
        "  # Make sure dimension of m, and columns of X_data_matrix are the same\n",
        "  if len(m) != X_data_matrix.shape[1]:\n",
        "    warnings.warn(\"Dimensions of m and X_data_matrix do not match.\")\n",
        "    sys.exit(1)\n",
        "  # reassign length of m to P if this passes.. issues with len(m) not acting right\n",
        "  p = X_data_matrix.shape[1]\n",
        "  # Check dimensions of the betas(given by m), and the Sigma covariance matrices\n",
        "  # Initiate the IMCV and MC mean list\n",
        "  mu_IMCV_list = []\n",
        "  mu_MC_list = []\n",
        "  # Initialize the beta-hat-transformation constant. Needed to transform samples generated from EM mixture to beta\n",
        "  beta_hat_operator = scipy.linalg.fractional_matrix_power(X_data_matrix.T @ X_data_matrix, -1/2)\n",
        "  for i in range(T_iterations):\n",
        "    # Initiate X chain, Y chain, and alpha chain\n",
        "    ## X is current state\n",
        "    ## Y is proposed state\n",
        "    ## Alpha chain is the ratio of the pdfs to see if the sample should be accepted: we will also generate uniform RV for acceptance calculation\n",
        "    X_chain = []\n",
        "    Y_chain = []\n",
        "    alpha_chain = []\n",
        "    for j in range(N_samples):\n",
        "      # edge case: X is empty\n",
        "      if len(X_chain) == 0:\n",
        "        # Accept proposed samples from mixed EM Gaussian mixture with probability 1\n",
        "        # Sample from mixture first, and then draw from the chosen distribution\n",
        "        chosen_g = np.random.choice(a = g_j_hat, p = weights)\n",
        "        Y_i = scipy.stats.multivariate_normal.rvs(mean = [0 for _ in range(p)], cov = chosen_g * np.identity(p))\n",
        "        ### Transform sample\n",
        "        Y_i = np.dot(beta_hat_operator, Y_i)\n",
        "        ### Append to X_chain\n",
        "        X_chain.append(np.asarray(Y_i))\n",
        "        ### Append to Y_chain\n",
        "        Y_chain.append(np.asarray(Y_i))\n",
        "        ### Append to Alpha Chain\n",
        "        alpha_chain.append(1)\n",
        "      else:\n",
        "        # Simulate Y_i first and perform transformation\n",
        "        ## Sample g from mixture of gaussians, and then sample from that distribution\n",
        "        chosen_g = np.random.choice(a = g_j_hat, p = weights)\n",
        "        Y_i = scipy.stats.multivariate_normal.rvs(mean = [0 for _ in range(p)], cov = chosen_g * np.identity(p))\n",
        "        Y_i = np.dot(beta_hat_operator, Y_i)\n",
        "        # Append to proposal chain and get current state(X_i)\n",
        "        Y_chain.append(np.asarray(Y_i))\n",
        "        X_i = X_chain[-1]\n",
        "        # Calculate pseudo-prior for acceptance/rejection sampling\n",
        "        ## Note that I sample the g-priors first, to ensure that both the pseudo-prior for X_i and Y_i are calculated from the same sets of g-priors\n",
        "        g_priors_for_pseudo_prior = scipy.stats.lomax(c = 1).rvs(size = L)\n",
        "        alpha_i_numerator = (linear_regression_pseudo_prior_estimation(Y_i, L, X_data_matrix, g_priors_for_pseudo_prior) *\n",
        "                   np.dot(weights, [scipy.stats.multivariate_normal.pdf(x = X_i, mean = [0 for _ in range(p)], cov = g_i * scipy.linalg.inv(X_data_matrix.T @ X_data_matrix)) for g_i in g_j_hat]))\n",
        "        alpha_i_denominator = (linear_regression_pseudo_prior_estimation(X_i, L, X_data_matrix, g_priors_for_pseudo_prior) *\n",
        "                   np.dot(weights, [scipy.stats.multivariate_normal.pdf(x = Y_i, mean = [0 for _ in range(p)], cov = g_i * scipy.linalg.inv(X_data_matrix.T @ X_data_matrix)) for g_i in g_j_hat]))\n",
        "        alpha_i = min(1, alpha_i_numerator/alpha_i_denominator)\n",
        "        alpha_chain.append(alpha_i)\n",
        "        # acceptance rejection\n",
        "        U = np.random.uniform()\n",
        "        if U < alpha_i:\n",
        "          X_chain.append(np.asarray(Y_i))\n",
        "        else:\n",
        "          X_chain.append(np.asarray(X_i))\n",
        "    # Calculate the estimator means\n",
        "    ## Returning the same values for all of the terms here. Big issue with this function. Need some dependence of F function (marginal likelihood of model) on current states\n",
        "    ## Y and X\n",
        "    mu_MC_i = np.mean([conditional_y_on_beta(X_chain[i], X_data_matrix, Y_response_vector, sigma_error) for i in range(len(X_chain))])\n",
        "    mu_MC_list.append(float(mu_MC_i))\n",
        "    ### Perhaps we induce dependence on the current states of the chains (X,Y) by sampling sigma (like a Gibbs sampler)?\n",
        "    mu_IMCV_i = np.mean([conditional_y_on_beta(X_chain[i], X_data_matrix, Y_response_vector, sigma_error) +\n",
        "        alpha_chain[i] * (conditional_y_on_beta(Y_chain[i], X_data_matrix, Y_response_vector, sigma_error) - conditional_y_on_beta(X_chain[i], X_data_matrix, Y_response_vector, sigma_error))\n",
        "        - (conditional_y_on_beta(Y_chain[i], X_data_matrix, Y_response_vector, sigma_error) - expectation_f_function_y_likelihood(weights, g_j_hat, X_data_matrix, Y_response_vector, sigma_error))\n",
        "    for i in range(len(Y_chain))])\n",
        "    mu_IMCV_list.append(float(mu_IMCV_i))\n",
        "  return mu_MC_list, mu_IMCV_list"
      ],
      "metadata": {
        "id": "bQaRVG5fkg3w"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check here\n",
        "sample = X_data_matrix_from_m(X, m = \"123\")\n",
        "\n",
        "m = \"123\"\n",
        "print([0 for _ in range(len(m))])\n",
        "chosen_g = np.random.choice(a = np.array(no_update_mu_diagonal_sigma[4]), p = np.array(no_update_mu_diagonal_sigma[0]))\n",
        "print(no_update_mu_diagonal_sigma[4] @ no_update_mu_diagonal_sigma[0])\n",
        "print([float(scipy.stats.invgamma.rvs(a = 1/2, scale = 1/2)) for i in range(2)])\n",
        "sample.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPN5_38vkj8U",
        "outputId": "01d95e8f-aa65-41c5-9643-ef6fe0493113"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0]\n",
            "6.637078616112744\n",
            "[2.075729078210524, 0.7018666595325475]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuing the m = \"123\" example as a sample test to see if our code works -- trying to recover the numbers from the paper\n",
        "test_run = multivariate_linear_regression_metropolis(4, 50, 50, np.asarray(no_update_mu_diagonal_sigma[0]).ravel(), np.asarray(no_update_mu_diagonal_sigma[4]).ravel(),\n",
        "                                                     \"123\", scipy.stats.lomax(c=1), X_data_matrix_from_m(X, m = \"123\"), y, 100, 2.5)"
      ],
      "metadata": {
        "id": "KffH_evXkl8A"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu_IMCV_test = test_run[1]\n",
        "print(np.mean([-math.log10(mu_IMCV_test[i]) for i in range(len(mu_IMCV_test))]))\n",
        "print(np.mean([-math.log10(mu_MC_test[i]) for i in range(len(mu_MC_test))]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHEnMpmoknpp",
        "outputId": "f6c3dfd7-d4b1-4f6c-f50f-3cdc4a03097e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "86.89662465910158\n",
            "80.99970781520547\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.577193970255456e+52"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    }
  ]
}