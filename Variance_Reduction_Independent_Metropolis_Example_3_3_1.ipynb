{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIAY7pq9MOCv08MCuSWKRI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyhu25/Variance-Reduction-Independent-Metropolis/blob/main/Variance_Reduction_Independent_Metropolis_Example_3_3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOFoSKWJ_6r5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import random\n",
        "from numpy import linalg\n",
        "import math\n",
        "import scipy\n",
        "import scipy.stats\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import rv_continuous, rv_discrete\n",
        "from scipy.stats._distn_infrastructure import rv_frozen\n",
        "from scipy.special import logsumexp\n",
        "import scipy.integrate\n",
        "import warnings\n",
        "import sys\n",
        "import statistics\n",
        "import pandas as pd\n",
        "from IPython.display import display, Math, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Adaptive Independent Metropolis Sampler and Numerical Examples"
      ],
      "metadata": {
        "id": "znp2hAyMAL_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of Scheme\n"
      ],
      "metadata": {
        "id": "JtckD2DqANCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a sequence of independent Metropolis adaptations $\\left\\{ IM(P_{i}, \\pi, q_{\\theta_{i}}) \\right\\}_{i=1}^{\\infty}$ with proposal densities $\\left\\{q_{\\theta_{i}} \\right\\}_{i=1}^{\\infty}$, with parameters $\\theta_{i}$ drawn from a vector $ùöØ$.\n",
        "\n",
        "With this new scheme, we add an update function $H_{\\theta_{i}}(x)$ and stepsize $\\alpha_{i}$ for each step $i$. But for this schema to work, we do need to assume that every sampling chain has a state space $ùïè$ that allows the parameters of the proposal density to converge to some $\\theta^{*}$ ($\\displaystyle \\lim_{i \\to \\infty} \\theta_{i} = \\theta^{*}$ in probability for some $\\theta^{*} ‚àà ùöØ$).\n",
        "\n",
        "Ideally speaking, the ideal estimator of the $IM(P, \\pi, q)$ sampler is the ability to minimize the distance, in terms of the KL divergence, between the proposal density $q$ and the target distribution $\\pi$.  "
      ],
      "metadata": {
        "id": "4sHad5FtAPiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Adaptation based on KL divergence (work in progress)\n",
        "\n",
        "To obtain a good proposal density $q$ that is as close to possible, in terms of KL divergence, to the target $\\pi$, we will use a gradient direction function as our update function $H_{\\theta}$ to minmize the KL divergence $ùïÇùïÉ(q_{\\theta}(x)||\\pi(x))$. To obtain the sequence of adapted proposal distributions $q_{\\theta}(x)$, we update $\\theta_{i}$ using this form:\n",
        "\n",
        "$\\theta_{i+1} = \\theta_i - \\alpha_i \\nabla_{\\theta_{i}} \\mathrm{KL}(q_{\\theta_{i}}(x) \\,\\|\\, \\pi(x))$ where $\\nabla_{\\theta_{i}}ùîº(q_{\\theta_{i}}(x)||\\pi(x))$ = $\\nabla_{\\theta_{i}}ùîº_{q_{\\theta_{i}}}(log\\frac{q_{\\theta_{i}}(x)}{\\pi(x)})$\n",
        "\n",
        "with $\\alpha_{i} > 0$ is a step-size parameter.\n",
        "\n",
        "However, this optimization procedure is not feasible since the gradient is not available in closed form. Instead, we assume that the independent Metropolis proposal density $q_{\\theta_{i}}$ can be reparameterized from a simpler distribution $p(z)$, which allows us to use efficient reparameterization gradient methods, while ensuring that the estimates of the gradient method remain unbiased.\n",
        "\n",
        "Note that to remain reproducibility (in that I don't have to manually code the gradient of the distributions in the function for each example), I will use PyTorch for this function, as the distributions called from PyTorch have supported gradient function calls. However, PyTorch distribution objects do not have in-built expectation of function calls, something that SciPy objects have. I will call 2 different objects for $q$: a PyTorch distribution object for gradient calculation, and a Scipy distribution object for calculation of $ùîº(q_{\\theta_{i}}(x)||\\pi(x))$.\n"
      ],
      "metadata": {
        "id": "RIBmvz4ZARhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example 3.3.1: Adaptive IM with d-dimensional Gaussian target and Gaussian proposal\n",
        "\n",
        "Let our d-dimensional target be the Gaussian $\\pi(x) = N(x|0, I_{d})$ and adaptive proposal initialized as $q(x) = N(x| I_{d}, LL^{T})$ where $I_{d}$ is $d$-dimensional length of 1, and L has elements:\n",
        "\n",
        "$$\n",
        "L(i,j) =\n",
        "\\begin{cases}\n",
        "1, & \\text{if } i \\ge j \\\\\n",
        "0, & \\text{if } \\ i < j\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "For the unbiased gradient descent method, we can use the reparameterization trick to rewrite the proposal distribution $q$ in terms of a simpler transformation. In this case, let $z \\sim N(0_{d}, I_{d})$, where $0_{d}$ is\n",
        "\n",
        "$H_{\\theta}(Y) = \\nabla_{\\theta}ùîº_{q_{\\theta}(x)} log(q_{\\theta}(x)) - \\nabla_{\\theta}log \\pi(Y = \\mu + Lz), z \\sim N(0, I_{d})$, where the update is defined as:\n",
        "\n",
        "$\\theta_{i+1} ‚Üê \\theta_{i} -\\alpha_{i}H_{Œ∏_{i}}(Y_{i})$\n",
        "\n",
        "Note that $ùîº_{q_{\\theta}(x)}log(q_{\\theta}(x))$ is the negative entropy of the Gaussian distribution.\n",
        "\n",
        "The entropy derivation for both univariate and multivariate can be found [here](https://gregorygundersen.com/blog/2020/09/01/gaussian-entropy/).\n",
        "\n",
        "We will be updating the parameters $(\\mu, L)$ in this update function.\n",
        "\n",
        "For this example, we are using the generated samples from $q$ to estimate the coordinates of the mean vector of the target density $\\pi(x)$ (so estimating $\\mu_{\\pi}(x)$) using the generated samples. So the $F(x)$ function should be the identity function with respect to $x$."
      ],
      "metadata": {
        "id": "EFRQEhFiAVNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will rewrite the negative entropy in terms of the Cholesky factor $L$.\n",
        "\n",
        "$E_{q_{\\theta}(x)}log(q_{\\theta}(x)) = -H(x) = \\frac{D}{2}(1 + log(2\\pi)) + \\frac{1}{2}log (det(L L^{T}))$\n",
        "\n",
        "For the gradient of the negative entropy, below are the elementwise derivatives. Note that these schemes will be used for the update for the Adaptive Metropolis\n",
        "\n",
        "$-\\frac{\\partial H(x)}{\\partial \\mu} = 0$\n",
        "\n",
        "$-2 \\frac{\\partial H(x)}{ \\partial L} = -\\frac{\\partial}{\\partial Œ£}(log(det(Œ£))) \\frac{\\partial Œ£}{\\partial L} = -(\\Sigma^{-1})^{T} \\frac{\\partial Œ£}{\\partial L} = -\\Sigma^{T} \\frac{\\partial}{\\partial L}(LL^{T}) = -(LL^{T})^{-1}[\\frac{\\partial}{\\partial L}(L^{T}) + \\frac{\\partial}{\\partial L}^{T}(L)] \\\\\\ = -(LL^{T})^{-1}2L = -2(L^{T})^{-1}$\n",
        "\n",
        "And so $\\frac{\\partial H(x)}{\\partial L} = (L^{T})^{-1}$\n",
        "\n",
        "Note that since $\\Sigma$ is symmetric, $\\Sigma$ is positive semi-definite, and thus $det(\\Sigma)$ = $|det(\\Sigma)|$\n",
        "\n",
        "According to the [Matrix Differential Calculus book](https://nzdr.ru/data/media/biblio/kolxoz/M/MA/MAl/Magnus%20J.,%20Neudecker%20H.%20Matrix%20differential%20calculus%20with%20applications%20in%20statistics%20and%20econometrics%20(3ed.,%20Wiley,%201999)(ISBN%200471986321)(O)(470s)_MAl_.pdf#page=191), we can represent the derivative of $log(det(\\Sigma))$ in terms of a trace of its differential and inverse, if we verify that matrix $F = Œ£$ is $k$-times continuously differentiable on its domain $S \\subset ‚Ñù^{m \\times m}$, where $m$ is the dimension of $\\Sigma$, and also $log|F| = |log F|$.\n",
        "\n",
        "Note that in the book, $F$ is treated as a function. Technically speaking, all matrices can be viewed as a linear operator between two spaces. However, in this example, we are simply treating the matrix as itself, and nothing else. Still, this theorem holds.\n",
        "\n",
        "From the trace trick,\n",
        "\n",
        "$-2\\frac{\\partial H(x)}{\\partial L} = -2 \\frac{\\partial H(x)}{\\partial \\Sigma} \\frac{\\partial \\Sigma}{\\partial L} = -tr(Œ£^{-1}\\frac{\\partial \\Sigma}{\\partial L}) = -tr(Œ£^{-1}[\\frac{\\partial}{\\partial L}(LL^{T})])$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6r1Xd79sCy7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k2egB0i8VjAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cholesky_negative_entropy_calculation(L):\n",
        "  return np.linalg.inv(L.T)"
      ],
      "metadata": {
        "id": "_FpMGdiEkxGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c0oZq66OgJ2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multivariate_general_batch_adaptive_gaussian_IMCV_sampler(initial_mu, initial_L, B_batches, l_num_batches, alphas, coefficient_calculation):\n",
        "  # Sanity check mu and L (Cholesky of proposal covariance matrix) to see if dimensions match\n",
        "  if len(initial_mu) != initial_L.shape[0] or len(initial_mu) != initial_L.shape[1]:\n",
        "    print(\"Dimensions of initial_mu and L(Cholesky factor for covariance matrix) does not match\")\n",
        "    sys.exit(1)\n",
        "  # Check to see if length of alphas and number of iterations match\n",
        "  if len(alphas) != B_batches:\n",
        "    print(\"Length of alphas and number of iterations do not match\")\n",
        "    sys.exit(1)\n",
        "  # Initialize list of mus for sampling purposes -- not to be confused with the mu of the gaussian proposal desntiy\n",
        "  mu_IMCV = []\n",
        "  mu_IMCV_coefficients = []\n",
        "  # Need dimension of mu and sigma -- we call this d\n",
        "  d = len(initial_mu)\n",
        "  # Initiate the initial parameters (mu, sigma/L) of the proposal -- will need to update throughout the batches\n",
        "  mu_loop = initial_mu\n",
        "  L_loop = initial_L\n",
        "  for i in range(l_num_batches):\n",
        "    # Initialize the Chain of the sampler\n",
        "    # X_chain is the current state of chain\n",
        "    # Y-chain is proposed state of chian\n",
        "    X_chain = []\n",
        "    Y_chain = []\n",
        "    alpha_chain = []\n",
        "    acceptance_counter = 0\n",
        "    for j in range(B_batches):\n",
        "      if len(X_chain) == 0:\n",
        "        # Sample and accept with probability 1\n",
        "        ## Using the form in the paper (y = mu + Lz, where z is multivariate standard normal)\n",
        "        Y_i = mu_loop + L_loop @ scipy.stats.multivariate_normal.rvs(mean = [0 for _ in range(d)], cov = np.eye(d))\n",
        "        X_chain.append(np.asarray(Y_i))\n",
        "        Y_chain.append(np.asarray(Y_i))\n",
        "        alpha_chain.append(1)\n",
        "      else:\n",
        "        # Propose sample\n",
        "        ## Using the form in the paper (y = mu + Lz, where z is multivariate standard normal)\n",
        "        Y_i = mu_loop + L_loop @ scipy.stats.multivariate_normal.rvs(mean = [0 for _ in range(d)], cov = np.eye(d))\n",
        "        Y_chain.append(Y_i)\n",
        "        # Acceptance rejection\n",
        "        X_i = X_chain[-1]\n",
        "        # acceptance ratio\n",
        "        alpha_i_numerator = (scipy.stats.multivariate_normal.pdf(x = Y_i, mean = np.asarray([0 for _ in range(d)]), cov = np.eye(d)) *\n",
        "              scipy.stats.multivariate_normal.pdf(x = X_i, mean = mu_proposal, cov = sigma_proposal))\n",
        "        alpha_i_denominator = (scipy.stats.multivariate_normal.pdf(x = X_i, mean = np.asarray([0 for _ in range(d)]), cov = np.eye(d)) *\n",
        "              scipy.stats.multivariate_normal.pdf(x = Y_i, mean = mu_proposal, cov = sigma_proposal))\n",
        "        alpha_i = min(1, alpha_i_numerator/alpha_i_denominator)\n",
        "        alpha_chain.append(alpha_i)\n",
        "        U = np.random.uniform()\n",
        "        if U < alpha_i:\n",
        "          X_chain.append(Y_i)\n",
        "          acceptance_counter += 1\n",
        "        else:\n",
        "          X_chain.append(X_i)\n",
        "    # End of batch mu calculations and parameter update\n",
        "    mu_IMCV_i =\n",
        "    if coefficient_calculation == True:\n",
        "      print(\"Work in Progress\") # Come back to work on this\n",
        "    # Update function\n",
        "    mu_loop =\n",
        "    L_loop =\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ky5M1_c_AagU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}